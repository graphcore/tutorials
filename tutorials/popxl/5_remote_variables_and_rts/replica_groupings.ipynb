{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4247b5d3",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fec1aca",
   "metadata": {},
   "source": [
    "# ReplicaGrouping\n",
    "\n",
    "In this short complementary notebook we are going to explain the concept of **ReplicaGrouping**. You can read more on the [official user guide](https://docs.graphcore.ai/projects/popxl/en/3.1.0/replication.html?highlight=replica%20grouping#replica-grouping).\n",
    "\n",
    "Generally speaking, a `ReplicaGrouping` object defines a group of IPUs.\n",
    "Replica groupings are used in two different contexts:\n",
    "- **Variables** : the replica group of a variable identifies the IPUs over which the variable is the same. The variable assumes the same value on IPUs that belongs to the same group.\n",
    "- **Collectives** : in collectives, replica groups are used to define which IPUs need to communicate. Communication happens between IPUs in the same group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f762377",
   "metadata": {},
   "source": [
    "## Definition\n",
    "A ReplicaGrouping is specified using a `group_size` and a `stride`.\n",
    "- The **group_size** tells how many IPUs are in the group.\n",
    "- The **stride** defines which IPUs are in the same group. Specifically, it tells the distance between IPUs belonging to the same group.\n",
    "\n",
    "To create a replica group, you can use the following syntax:\n",
    "```python\n",
    "ir = popxl.Ir(replication=replicas)\n",
    "rg = ir.replica_grouping(stride=1, group_size=4)\n",
    "```\n",
    "\n",
    "Given a certain group_size, \n",
    "\n",
    "$$\\text{num_groups} = \\text{replicas} // \\text{group_size}$$\n",
    "\n",
    "will be created.\n",
    "\n",
    "Since this must be an integer, not all values of parameters are allowed.\n",
    "\n",
    "You don't need to specify both the stride and the group_size.\n",
    "- If you only specify the stride, the group_size is assumed to be `group_size = replicas//stride`.\n",
    "- If you only specify the group_size, a unitary stride is assumed.\n",
    "- The **default** group, created with `ir.replica_grouping()` is *all replicas in the same group*: a single group with `group_size=replicas` and `stride=1`.\n",
    "\n",
    "A ReplicaGrouping defines an **assignment**. When a group is created, each IPU is given a number `0, ..., num_groups`. IPUs with the same number belong to the same group.\n",
    "\n",
    "Depending on your tastes, it may be more intuitive to think about groups in terms of assignments or in terms of IPU indices. \n",
    "In the cells below, we provide a short snippet that leverage both visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6887dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import popxl\n",
    "import numpy as np\n",
    "import popxl_addons as addons\n",
    "from typing import Optional\n",
    "from popxl import ReplicaGrouping\n",
    "from popxl import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a996e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rg(ipu_indices, rg):\n",
    "    print(\"#######################################\")\n",
    "    print(rg)\n",
    "    print(\"Assignment: \",rg.assignment)\n",
    "    for i in range(rg.num_groups):\n",
    "        print(f\" Group {i} : \", ipu_indices[np.where(np.asarray(rg.assignment)==i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffc4c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "ReplicaGrouping(num_replicas=8, stride=1, group_size=4, num_groups=2)\n",
      "Assignment:  [0, 0, 0, 0, 1, 1, 1, 1]\n",
      " Group 0 :  [0 1 2 3]\n",
      " Group 1 :  [4 5 6 7]\n",
      "#######################################\n",
      "ReplicaGrouping(num_replicas=8, stride=2, group_size=2, num_groups=4)\n",
      "Assignment:  [0, 1, 0, 1, 2, 3, 2, 3]\n",
      " Group 0 :  [0 2]\n",
      " Group 1 :  [1 3]\n",
      " Group 2 :  [4 6]\n",
      " Group 3 :  [5 7]\n",
      "#######################################\n",
      "ReplicaGrouping(num_replicas=8, stride=4, group_size=2, num_groups=4)\n",
      "Assignment:  [0, 1, 2, 3, 0, 1, 2, 3]\n",
      " Group 0 :  [0 4]\n",
      " Group 1 :  [1 5]\n",
      " Group 2 :  [2 6]\n",
      " Group 3 :  [3 7]\n",
      "#######################################\n",
      "ReplicaGrouping(num_replicas=8, stride=1, group_size=2, num_groups=4)\n",
      "Assignment:  [0, 0, 1, 1, 2, 2, 3, 3]\n",
      " Group 0 :  [0 1]\n",
      " Group 1 :  [2 3]\n",
      " Group 2 :  [4 5]\n",
      " Group 3 :  [6 7]\n",
      "#######################################\n",
      "ReplicaGrouping(num_replicas=8, stride=2, group_size=4, num_groups=2)\n",
      "Assignment:  [0, 1, 0, 1, 0, 1, 0, 1]\n",
      " Group 0 :  [0 2 4 6]\n",
      " Group 1 :  [1 3 5 7]\n"
     ]
    }
   ],
   "source": [
    "replicas = 8\n",
    "ir = popxl.Ir(replication=replicas)\n",
    "ipu_indices = np.arange(0, replicas)\n",
    "\n",
    "rg = ir.replica_grouping(stride=1, group_size=4)\n",
    "rg1 = ir.replica_grouping(stride=2, group_size=2)\n",
    "rg2 = ir.replica_grouping(stride=4)\n",
    "rg3 = ir.replica_grouping(group_size=2)\n",
    "rg4 = ir.replica_grouping(stride=2, group_size=4)\n",
    "\n",
    "print_rg(ipu_indices,rg)\n",
    "print_rg(ipu_indices,rg1)\n",
    "print_rg(ipu_indices,rg2)\n",
    "print_rg(ipu_indices,rg3)\n",
    "print_rg(ipu_indices,rg4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986e3e30",
   "metadata": {},
   "source": [
    "## Transpose of a ReplicaGrouping\n",
    "\n",
    "ReplicaGrouping have `.transpose()` method that generates another group.\n",
    "To understand what it is like, let's represent IPU indices as an array.\n",
    "\n",
    "<img src=\"images/ipu_indices.png\" alt=\"IPU indices\" style=\"width:500px;\"/>\n",
    "\n",
    "Defining a stride and a group size amounts to change this flat visualisation to a matrix visualisation where indices are arranged in `num_groups` columns and `group_size` row. The easiest way to build this matrix is looking at the assignment: the assigned ID denotes the column you have to place the IPU in. So if the assignment is `[0, 1, 0, 1, 0, 1, 0, 1]` so place IPUs 0,2,4,6 in column 1, IPUs 1,3,5,7 in column 1. If the assignment is `[0, 1, 2, 3, 0, 1, 2, 3]` you place IPUs 0,4 in column 0, IPUs 1,5 in column 1, IPUs 3,6 in column 2, IPUs 4,7 in column 3.\n",
    "\n",
    "Another easy way to build it is to place IPU indices in columns of length `group_size` so that between each adjacent indices there is `stride` distance: with a `stride=2, group_size=2`, you build the first column with indices 0,2, the second with indices 1,3 and so on.\n",
    "\n",
    "If you now take the transpose of this matrix, you get the transpose group.\n",
    "\n",
    "The image below illustrate this concept for a `ReplicaGrouping(stride=2, group_size=4)`.\n",
    "\n",
    "<img src=\"images/transpose.png\" alt=\"The transpose of a ReplicaGrouping\" style=\"width:500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a075e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "ReplicaGrouping(num_replicas=8, stride=2, group_size=4, num_groups=2)\n",
      "Assignment:  [0, 1, 0, 1, 0, 1, 0, 1]\n",
      " Group 0 :  [0 2 4 6]\n",
      " Group 1 :  [1 3 5 7]\n",
      "#######################################\n",
      "ReplicaGrouping(num_replicas=8, stride=1, group_size=2, num_groups=4)\n",
      "Assignment:  [0, 0, 1, 1, 2, 2, 3, 3]\n",
      " Group 0 :  [0 1]\n",
      " Group 1 :  [2 3]\n",
      " Group 2 :  [4 5]\n",
      " Group 3 :  [6 7]\n"
     ]
    }
   ],
   "source": [
    "replicas = 8\n",
    "ir = popxl.Ir(replication=replicas)\n",
    "ipu_indices = np.arange(0, replicas)\n",
    "\n",
    "rg = ir.replica_grouping(stride=2, group_size=4)\n",
    "rg_t = rg.transpose()\n",
    "\n",
    "print_rg(ipu_indices,rg)\n",
    "print_rg(ipu_indices,rg_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dbce70",
   "metadata": {},
   "source": [
    "## ReplicaGrouping for variables\n",
    "\n",
    "As mentioned in the introduction, when you create a variable in `popxl` you can specify its replica_grouping:\n",
    "```python\n",
    "var = popxl.variable(\n",
    "        ...\n",
    "        replica_grouping=ir.replica_grouping(group_size=2),\n",
    "    )\n",
    "```\n",
    "If no group is specified, the **default** group is used, which means the variable is assumed to be the same on all replicas.\n",
    "\n",
    "In `popxl-addons`, factories carry the information about the variable replica group. In this way, a variable with the appropriate group will be created on initialisation.\n",
    "Each individual factory has a `replica_group` property, and `NamedVariableFactories` have an associated `NamedReplicaGrouping` collection accessible via the `factories.replica_groupings` property.\n",
    "\n",
    "Likewise, all `add_variable_input` functions used to add variables in addons modules admit a replica_grouping parameter.\n",
    "\n",
    "### Example: simple data parallelism\n",
    "When implementing data parallelism with replication, all replicas have the same weights. Therefore, the replica grouping of variables is the default group, `ReplicaGrouping(stride=1, group_size=DP)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fc781",
   "metadata": {},
   "source": [
    "### Example: data parallelism + tensor parallelism\n",
    "We've not explored tensor parallelism in an tutorial. Hopefully a comprehensive understanding is not necessary for the purpose of this example.\n",
    "\n",
    "In short, tensor parallelism consists in splitting some weights tensors (that are too big to fit in memory), do computations with the sharded tensors and use collectives to reconstruct the full result after.\n",
    "\n",
    "For the purpose of this tutorial, just think about a linear layer that performs matrix multiplication.\n",
    "$$\n",
    "\\mathbf{A} = \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "c & d \n",
    "\\end{pmatrix}, \\quad\n",
    "\\vec{x} = \\begin{pmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "\\end{pmatrix}\n",
    " \\\\\n",
    "\\vec{y} = \\vec{x} \\cdot \\mathbf{A} = \\begin{pmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "c & d \n",
    "\\end{pmatrix} =  \\begin{pmatrix}\n",
    "x_1\\cdot a + x_2\\cdot c & x_1\\cdot b + x_2\\cdot d \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "y_1 & y_2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "The same result can be obtained by splitting (**sharding**) the matrix $\\mathbf{A}$ *column-wise* and then **gathering** the results.\n",
    "$$\n",
    "y_1 = \n",
    "\\begin{pmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "a \\\\\n",
    "c\n",
    "\\end{pmatrix} = x_1\\cdot a + x_2\\cdot c\n",
    "$$\n",
    "$$\n",
    "y_2 =\n",
    "\\begin{pmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "b \\\\\n",
    "d\n",
    "\\end{pmatrix} = x_1\\cdot b + x_2\\cdot d\n",
    "$$\n",
    "$$\n",
    "\\text{gather} \\to \\begin{pmatrix}\n",
    "y_1 & y_2 \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "x_1\\cdot a + x_2\\cdot c & x_1\\cdot b + x_2\\cdot d \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "In passing, notice that another possible splitting is a *row-wise* sharding of $\\mathbf{A}$ combined with a *row-wise* sharding of the input $\\vec{x}$ and an **all reduce** operation at the end.\n",
    "$$\n",
    " \\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "a & b \\\\\n",
    "\\end{pmatrix} =  \\begin{pmatrix}\n",
    "x_1 \\cdot a & x_1 \\cdot b\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    " \\begin{pmatrix}\n",
    "x_2 \\\\\n",
    "\\end{pmatrix} \\begin{pmatrix}\n",
    "c & d \\\\\n",
    "\\end{pmatrix} =  \\begin{pmatrix}\n",
    "x_2 \\cdot c & x_2 \\cdot d\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\text{all reduce} \\to \\begin{pmatrix}\n",
    "x_1\\cdot a + x_2 \\cdot c & x_1\\cdot b + x_2\\cdot d \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We can implement tensor parallelism using replication and letting each replica hold a different shard of the weight matrix. In the above example for column-wise sharding, we need two replicas (`TP` replicas = 2): one replica can have the `(a,c)` slice and the other one the `(b, d)` slice. The model spans across 2 IPUs.\n",
    "\n",
    "If we combine this with data parallelism over other 4 replicas (`DP` replicas = 4), we replicate this 2-IPUs model 4 times, using 8 IPUs in total.\n",
    "\n",
    "The ReplicaGrouping for the weight will be `ReplicaGrouping(stride=TP, group_size=DP)=ReplicaGrouping(stride=2, group_size=4)`, with an assignment `[0 1 0 1 0 1 0 1]` . This means that IPUs 0,2,4,6 have the same first shard, and IPUs 1,3,5,7 have the same second shard.\n",
    "\n",
    "<img src=\"images/variables_tp_dp.png\" alt=\"Variables grouping for a combination of TP and DP\" style=\"width:300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a1dd1",
   "metadata": {},
   "source": [
    "## ReplicaGrouping in collectives\n",
    "\n",
    "When using collectives the replica group defines the underlying **communication group**: communication happens only between IPUs that are in the same group.\n",
    "\n",
    "All `popxl` collectives have a `group` parameter that allows to specify the ReplicaGrouping for the communication.\n",
    "If no group is specified, the default group is used, meaning communication happens between all replicas.\n",
    "\n",
    "In `popxl-addons` there are extra utilities related to collectives in `rts`, `remote` and collectives custom ops. They all admit specification of one or more replica groups.\n",
    "\n",
    "The relation between the variable replica group and the communication group\n",
    "\n",
    "Notice that in several situation the transpose of a replica group is used in collectives. For example, in the above example about Tensor Parallelism + Data Parallelism, the replica group for the gather collective is the transpose group of the variable replica grouping, because we want to gather together *different* shards.\n",
    "\n",
    "<img src=\"images/collectives.png\" alt=\"Replica grouping for TP collectives\" style=\"width:500px;\"/>\n",
    "\n",
    "However, this is not a general rule. For example, you will see in the `mnist.ipynb` notebook that replicated tensor sharding collectives use the variable group as communication group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad9f39",
   "metadata": {},
   "source": [
    "## Example: Tensor Parallelism + Data Parallelism Linear Layer\n",
    "We now implement a small example of a Linear Layer executed using tensor parallelism (column-wise) and data parallelism, to illustrate the concepts in practice.\n",
    "You will see:\n",
    "- How replica groupings can be used in `addons` modules to create variables\n",
    "- How to initialise each replica with different data.\n",
    "- How replica groupings can be used in collectives.\n",
    "- How the `transpose` group can be used in tensor parallel collectives.\n",
    "\n",
    "We will use the handy `print_tensor` operation to print tensor values on all replicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdf9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constant_weight(tp, in_features, shard_size):\n",
    "    shards = [np.full((in_features,shard_size), i+1, dtype=np.float32) for i in range(tp)]\n",
    "    return shards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc28bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, tp: int, dp: int):\n",
    "        super().__init__()\n",
    "        self.shard_size = out_features//tp\n",
    "        self.replica_grouping = popxl.gcg().ir.replica_grouping(stride=tp, group_size=dp)\n",
    "        self.tp = tp\n",
    "        \n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # using iter initialise each replica with different data\n",
    "        w = self.add_variable_input(\"weight\",\n",
    "                                    iter(constant_weight(self.tp,x.shape[-1],self.shard_size)),\n",
    "                                    x.dtype,\n",
    "                                    replica_grouping=self.replica_grouping)\n",
    "        print(\"sharded weight shape: \", w.shape)\n",
    "        ops.print_tensor(w, title='w_shards', digits=2)\n",
    "        # sharded computation\n",
    "        y = x @ w\n",
    "        print(\"y shard shape: \", y.shape)\n",
    "        ops.print_tensor(y, title='y_shards',digits=2)\n",
    "        # gather shards in the transpose group\n",
    "        y_full = ops.collectives.replicated_all_gather(y, group=self.replica_grouping.transpose(),axis=-1)\n",
    "        print(\"y full shape: \", y_full.shape)\n",
    "\n",
    "        ops.print_tensor(y_full, title='y_full', digits=2)\n",
    "\n",
    "        return y_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3a9b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to project a vector (2,) to a vector (4,) with a matmul with weight of shape (2,4)\n",
    "# using tensor parallelism, hence splitting the weight in two shards of shape (2,2).\n",
    "TP=2\n",
    "DP=2\n",
    "out_features = 4\n",
    "in_features = 2\n",
    "x_data = np.ones((in_features), np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b6fd417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full tensor shape:  (2, 4)\n",
      "Full tensor: \n",
      "[[1. 1. 2. 2.]\n",
      " [1. 1. 2. 2.]]\n",
      "Shard (column wise) shape:  (2, 2)\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "[[2. 2.]\n",
      " [2. 2.]]\n",
      "Matmul full\n",
      "[2. 2. 4. 4.]\n",
      "Matmul sharded\n",
      "   y_shard_0:  [2. 2.]\n",
      "   y_shard_1:  [4. 4.]\n",
      "Gathered:  [2. 2. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "# numpy version\n",
    "shards = constant_weight(TP, in_features, out_features//TP)\n",
    "full_tensor = np.concatenate(shards, axis=-1)\n",
    "print(\"Full tensor shape: \", full_tensor.shape)\n",
    "print(\"Full tensor: \")\n",
    "print(full_tensor)\n",
    "print(\"Shard (column wise) shape: \", shards[0].shape)\n",
    "for s in shards:\n",
    "    print(s)\n",
    "print(\"Matmul full\")\n",
    "print(x_data @ full_tensor)\n",
    "print(\"Matmul sharded\")\n",
    "y_shards = []\n",
    "for i,s in enumerate(shards):\n",
    "    y_shard = x_data @ s\n",
    "    print(f\"   y_shard_{i}: \", y_shard)\n",
    "    y_shards.append(y_shard)\n",
    "print(\"Gathered: \", np.concatenate(y_shards, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c95e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharded weight shape:  (2, 2)\n",
      "y shard shape:  (2,)\n",
      "y full shape:  (4,)\n",
      "replica groupings in factories: \n",
      "NamedReplicaGrouping({\n",
      "weight: ReplicaGrouping(num_replicas=4, stride=2, group_size=2, num_groups=2),\n",
      "})\n",
      "assignment for weight: \n",
      "[0, 1, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile-tile it was set to 1471)\n",
      "w_shards_replica_0: [\n",
      " [1.0 1.0]\n",
      " [1.0 1.0]\n",
      "]\n",
      "\n",
      "w_shards_replica_1: [\n",
      " [2.0 2.0]\n",
      " [2.0 2.0]\n",
      "]\n",
      "\n",
      "w_shards_replica_2: [\n",
      " [1.0 1.0]\n",
      " [1.0 1.0]\n",
      "]\n",
      "\n",
      "w_shards_replica_3: [\n",
      " [2.0 2.0]\n",
      " [2.0 2.0]\n",
      "]\n",
      "\n",
      "y_shards_replica_0: [2.0 2.0]\n",
      "\n",
      "y_shards_replica_1: [4.0 4.0]\n",
      "\n",
      "y_shards_replica_2: [2.0 2.0]\n",
      "\n",
      "y_shards_replica_3: [4.0 4.0]\n",
      "\n",
      "y_full_replica_0: [2.0 2.0 4.0 4.0]\n",
      "\n",
      "y_full_replica_1: [2.0 2.0 4.0 4.0]\n",
      "\n",
      "y_full_replica_2: [2.0 2.0 4.0 4.0]\n",
      "\n",
      "y_full_replica_3: [2.0 2.0 4.0 4.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ir = popxl.Ir(replication=TP*DP)\n",
    "\n",
    "\n",
    "with ir.main_graph:\n",
    "    x = popxl.constant(np.ones((in_features)), popxl.float32)\n",
    "    facts, linear = Linear(out_features, TP, DP).create_graph(x)\n",
    "    print(\"replica groupings in factories: \")\n",
    "    print(facts.replica_groupings)\n",
    "    print(\"assignment for weight: \")\n",
    "    print(facts.replica_groupings.weight.assignment)\n",
    "    vars = facts.init()\n",
    "    y, = linear.bind(vars).call(x)\n",
    "\n",
    "with popxl.Session(ir,'ipu_hw') as session:\n",
    "    session.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdac741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
