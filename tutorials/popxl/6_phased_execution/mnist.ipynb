{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb381e4",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db79ce",
   "metadata": {},
   "source": [
    "# Phased Execution\n",
    "\n",
    "Phased execution is an execution strategy that can be applied when the whole model is too big to fit in memory.\n",
    "\n",
    "When you design a phased execution strategy, you partition the graph execution into **phases**. Variables and required activations for the phases are stored remotely in the streaming memory.\n",
    "When the phase needs to be executed, variables and needed activations are loaded. This way tensors are only required to be alive during the phase execution.\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/phase_diagram.jpg\" style=\"width:400px;\"/>\n",
    "<figcaption> <b>Fig 1: </b> Execution diagram for a phase, consisting of <code> load </code> -> <code>compute</code> -> <code>store</code> steps. Load and store operations can be towards the streaming memory (remote load/store) or towards the host.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/phased.jpg\" style=\"width:700px;\"/>\n",
    "<figcaption> <b>Fig 2: </b> On the right, non phased model: all variables and activations are alive for the whole duration of the program.  On the left, an example of phased execution with three phases, corresponding to the three layers of the model. Variables and activations for each phase are stored remotely and only loaded when the layer needs to execute. The lower section of the image shows the difference in memory occupation (just an example) due to reduced liveness of variables and activations. Backward is not shown.\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d7844",
   "metadata": {},
   "source": [
    "## Batch serialisation\n",
    "\n",
    "The building block of phased execution is the phase graph, made of its `load` -> `compute` -> `store` steps. \n",
    "Typically phased execution is implemented together with gradient accumulation technique and the phase graph needs to be repeated in the gradient accumulation loop. \n",
    "The naive implementation of the gradient accumulation loop would look like:\n",
    "\n",
    "```python\n",
    "for _ in range(gradient_accumulation_steps):\n",
    "    ...\n",
    "    # ---- Layer A ----\n",
    "    \n",
    "    a_vs = load() # load layer specific variables \n",
    "    a_xs = load() # load inputs\n",
    "    a_ys = a.bind(vs).call(xs) \n",
    "    store(a_ys) # store activations\n",
    "\n",
    "    # ---- Layer B ----\n",
    "    \n",
    "    b_vs = load() # load layer specific variables \n",
    "    b_xs = load() # load inputs, that are the previous layer activations a_ys\n",
    "    b_ys = b.bind(vs).call(b_xs)\n",
    "    store(b_ys) # store activations\n",
    "    ...\n",
    "optimiser\n",
    "```\n",
    "\n",
    "However, variables are updated only after the gradient accumulation loop. Hence, they can be loaded once for all to reduce the number of load operations and the communication cost.\n",
    "A better loop for phased execution is \n",
    "\n",
    "```python\n",
    "...\n",
    "# ---- Layer A ----\n",
    "\n",
    "a_vs = load() # load layer specific variables \n",
    "for _ in range(gradient_accumulation_steps):\n",
    "    a_xs = load() # load inputs\n",
    "    a_ys = a.bind(vs).call(a_xs)\n",
    "    store(a_ys) # store activations. Note that we now need to store activations for each step!\n",
    "\n",
    "# Layer B\n",
    "b_vs = load() # load layer specific variables \n",
    "for _ in range(gradient_accumulation_steps):\n",
    "    b_xs = load() # load inputs, that are previous layer activations for the same GA step, a_ys[step]\n",
    "    b_ys = b.bind(vs).call(b_xs)\n",
    "    store(b_ys) # store activations\n",
    "...\n",
    "optimiser\n",
    "```\n",
    "Batch serialisation is a transform that takes a graph and build a `repeat` loop with this kind of structure:\n",
    "```\n",
    "load variables for the phase\n",
    "repeat(load activations - compute - store activations)\n",
    "... additional things (for example, optimizer)\n",
    "```\n",
    "\n",
    "Since phased execution makes use of remote buffers, you should use RTS to amortise the communication overhead, in which case you need to include the appropriate collective operations in the picture.\n",
    "\n",
    "Images below illustrate batch serialised phased execution.\n",
    "<figure>\n",
    "<img src=\"images/batch_serialisation.jpg\" style=\"width:700px;\"/>\n",
    "<figcaption> <b>Fig 3: </b> A batch serialised forward phase. On the right, RTS is included.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/batch_serialisation_grad.jpg\" style=\"width:700px;\"/>\n",
    "<figcaption> <b>Fig 4: </b> A batch serialised backward phase. On the right, RTS is included.\n",
    "</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686fa698",
   "metadata": {},
   "source": [
    "### Batch serialisation in popxl.addons\n",
    "It should be clear from the above code snippets that in batch serialisation we need to store activations for all steps.\n",
    "This is reflected in the remote buffers structure. In the context of `popxl.addons` batch serialisation, it's useful to consider remote buffers as if they were matrices, with rows corresponding to different phases that can share the same buffer (they need to have the same graph) and `steps` columns corresponding to the batch indices. Underneath this logical layout, each element at a position defined by row and column has its own entry in the underlying `popxl` remote buffer, consisting of `rows * steps` entries.\n",
    "\n",
    "<figure>\n",
    "<img src=\"images/remote_buffer.jpg\" style=\"width:700px;\"/>\n",
    "<figcaption> <b>Fig 5: </b> Remote buffer logical layout. Rows correspond to different phases that can share the same buffer (for example, when the phases are identical layers). Columns are the different batches, once for each gradient accumulation step. The entry index in the buffer is given by the <code> flat_index </code>.\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "You can create this kind of buffers with `batch_serial_buffer(t: popxl.Tensor, steps: int, rows: int = 1)`.\n",
    "\n",
    "To build a batch serialised graph in `popxl.addons` you can use `batch_serialise(...)` and `batch_serialise_fwd_and_grad(...)`\n",
    "\n",
    "```python\n",
    "batch_serialise(\n",
    "                    graph: GraphWithNamedArgs,\n",
    "                    steps: int,\n",
    "                    load_handles: Dict[popxl.Tensor, Union[popxl.HostToDeviceStream, RemoteBufferAndOffset]],\n",
    "                    store_streams: Dict[popxl.Tensor, popxl.DeviceToHostStream],\n",
    "                    store_buffers: Dict[popxl.Tensor, RemoteBufferAndOffset],\n",
    "                    seed_input: Optional[popxl.Tensor] = None,\n",
    "                    rows: int = 1,\n",
    "                    io_mode: Literal['compute', 'io', 'io_overlapped'] = 'io'\n",
    "                ) -> BatchSerialResult\n",
    "```\n",
    "You need to provide the `graph` which you want to repeat for `steps` times.\n",
    "\n",
    "Also, you need to tell how the inputs for the graph are loaded, providing a dictionary between each input and a host to device stream (if loaded from the host) or a `RemoteBufferAndOffset` which is just a `tuple` of a buffer and a row offset to access it.\n",
    "For example, if you created a buffer with \n",
    "```python\n",
    "x_buffer = batch_serial_buffer(first_layer_output, steps=opts.gradient_accumulation, rows=3)\n",
    "```\n",
    "you can then use it as a `load handle` for next layers:\n",
    "```python\n",
    "layer2_bs = batch_serialise(layer2, steps, {layer2.graph.inputs[0] : (x_buffer,0)}, ...)\n",
    "layer3_bs = batch_serialise(layer2, steps, {layer3.graph.inputs[0] : (x_buffer,1)}, ...)\n",
    "layer4_bs = batch_serialise(layer2, steps, {layer4.graph.inputs[0] : (x_buffer,2)}, ...)\n",
    "````\n",
    "`(x_buffer,0)` is a `RemoteBufferWithOffset`, telling that `layer2` input should be loaded from the `x_buffer` first row. Likewise, `layer3.graph.inputs[0] : (x_buffer,1)` specifies that `layer3` input should be loaded from row one. \n",
    "As you can see, you don't have to worry about the batch dimension, you always just need to think about the row you want to access. Internally, the graph will access the correct column at each step.\n",
    "If you don't provide a handle for a certain input, this will be an input of the batch serialised graph.\n",
    "\n",
    "After specifying the `load_handles`, you can provide `store_streams` and `store_buffers`for the layer outputs. They are kept separate because sometimes you may want to use both: stream an output to the host and store it in a buffer.\n",
    "Outputs that are not specified in `store_streams` or `store_buffers` are not outputs of the batch serialised graph.\n",
    "\n",
    "If your layer requires a different seed each time it's executed (for example, if you are using dropout), you should provide that input as `seed_input` parameter. That way a new random seed will be generated for each iteration step and fed to the layer graph.\n",
    "\n",
    "The `rows` parameter allows you to specify multiple rows.\n",
    "\n",
    "When you call a batch serialised graph, the first input is the row offset to access remote buffers, as in\n",
    "```python\n",
    "bs_graph = batch_serialise(graph, ..., rows=2)\n",
    "bs_graph.call(0) # access first row in the buffers\n",
    "bs_graph.call(1) # access second row in the buffers\n",
    "```\n",
    "\n",
    "Finally, the `io_mode` parameter manages how to load/store tensors during the loop.\n",
    "- `compute` uses the Compute tiles.\n",
    "- `io` uses the IO tiles.\n",
    "- `io_overlapped` uses the io tiles and builds the loop such that Compute and IO can execute at the same time.\n",
    "\n",
    "The `io` and `io_overlapped` modes require some tiles to be reserved as IO tiles (read also [popart user guide](https://docs.graphcore.ai/projects/popart-user-guide/en/3.1.0/overlap_io.html?highlight=io%20tiles#configuring-io-tiles)).\n",
    "You can do that by specifying session options after creating the `ir`.\n",
    "```python\n",
    "session_opts = ir._pb_ir.getSessionOptions()\n",
    "session_opts.numIOTiles = 32 \n",
    "```\n",
    "\n",
    "The `batch_serialise_fwd_and_grad` transform is very similar to the `batch_serialise` transform\n",
    "```python\n",
    "def batch_serialise_fwd_and_grad(\n",
    "        forward_graph: GraphWithNamedArgs,\n",
    "        gradient_graph: GraphWithNamedArgs,\n",
    "        named_inputs_for_grad_graph: NamedTensors,\n",
    "        steps: int,\n",
    "        load_handles: Dict[popxl.Tensor, Union[popxl.HostToDeviceStream, RemoteBufferAndOffset]],\n",
    "        store_streams: Dict[popxl.Tensor, popxl.DeviceToHostStream],\n",
    "        store_buffers: Dict[popxl.Tensor, RemoteBufferAndOffset],\n",
    "        seed_input: Optional[popxl.Tensor] = None,\n",
    "        rows: int = 1,\n",
    "        io_mode: Literal['compute', 'io', 'io_overlapped'] = 'io') -> Tuple[BatchSerialResult, BatchSerialResult]:\n",
    "```\n",
    "It applies batch serialisation to the both forward and backward graphs, ensuring that all inputs in the backward graph that derive from the forward are properly managed.\n",
    "This means that:\n",
    "- If there is a `store_buffer` for the tensor, this same buffer is used as a `load_handle` in the backward.\n",
    "- If there is a `load_handle` for the tensor, this `load_handle` is used\n",
    "- If the tensor is provided in `named_inputs_for_grad_graph`, the returned gradient graph will have this tensor as a named input and you will need to bind the graph to it. Typically, you want to use this parameter to provide the forward variables:\n",
    "```python\n",
    "fwd_vars = fwd_facts.init()\n",
    "bwd_vars = bwd_facts.init() # gradient accumulators in autodiff_with_accum\n",
    "bwd_vars.update(fwd_vars.copy())\n",
    "batch_serialised_bwd.graph.bind(bwd_vars).call(0) \n",
    "```\n",
    "- If the tensor is not provided in any of these ways, a new buffer is created for it, where the forward tensor will be stored.\n",
    "\n",
    "The result of a batch serialisation transform is a `BatchSerialResult`, gathering the batch serialised `graph`, `store_buffers` and a  dictionary to remap tensors from the original graph to the transformed one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f417eb",
   "metadata": {},
   "source": [
    "## Mnist with phased execution and batch serialisation\n",
    "In this tutorial we are going to implement a phased execution mnist example illustrating all these concepts.\n",
    "We will use data parallelism, remote buffers and replicated tensor sharding. Hence, check out the previous tutorials on these topics: \n",
    "- [Data parallelism and gradient accumulation](../3_data_parallelism)\n",
    "- [Remote variables and RTS](../5_remote_variables_and_rts)\n",
    "\n",
    "You may also want to have another look at outlining in the very [first tutorial](../1_basic_concepts).\n",
    "\n",
    "Our network has 4 layers. We define 7 phases for training:\n",
    "- fc1 forward\n",
    "- fc2 forward\n",
    "- fc3 forward\n",
    "- output layer fwd + loss + output layer bwd \n",
    "- fc3 backward\n",
    "- fc2 backward\n",
    "- fc1 backward\n",
    "\n",
    "Forward variables and optimizer state are stored remotely. \n",
    "We use a `Graphs` class to keep together the forward, backward, and optimizer graphs for the same module and easily deal with loading/storing/updating variables. \n",
    "We create three `Graphs` objects:\n",
    "- one for `fc1`, corresponding to a `Linear` module. In `input_layer_batch_serialise`, we batch serialise the forward and backward graphs for this layer, using `batch_serialise_fwd_and_grad`.\n",
    "- one for the inner layers,`fc2` and `fc3`, which are identical, corresponding to a `Linear` module with different input shape from the first one. Since they are identical, they share the same graph and they can use the same remote buffer for forward variables and optimizer state, but we need to specify `2` entries (the `Graphs` class has an `entries` parameter for this aim). In `layer_batch_serialise`, we batch serialise the forward and backward graphs, using `batch_serialise_fwd_and_grad`. We need to specify `rows=2` when using the transform, one row for each layer. \n",
    "- one corresponding to the `OutputLayerWithBwd` module. In this case, we don't have a backward graph: the forward graph already includes the backward. Hence, in `output_layer_batch_serialise`, we use the `batch_serialise` transform.\n",
    "\n",
    "We also need two buffers to connect the phases, a `x_buffer` and a` dx_buffer` where each phase can read its inputs and store its outputs.\n",
    "```python\n",
    "x_buffer = batch_serial_buffer(\n",
    "                               fc1.fwd.graph.outputs[0],\n",
    "                               steps=opts.train.gradient_accumulation,\n",
    "                               rows=num_inner_layers + 1\n",
    "                               )\n",
    "dx_buffer = batch_serial_buffer(\n",
    "                                fc1.bwd.graph.inputs[0],\n",
    "                                steps=opts.train.gradient_accumulation,\n",
    "                                rows=num_inner_layers + 1\n",
    "                                )\n",
    "```\n",
    "Each row in `x_buffer` corresponds to a forward phase excluding the last forward phase (which just reads from the buffer), and each row in `dx_buffer` correspond to a backward phase, excluding the last backward phase (which just reads from the buffer).\n",
    "\n",
    "The first forward phase loads inputs from host, and store its output in the first row of the `x_buffer`.\n",
    "Next forward phases load their inputs from the previous phase row, and store their output in their own row.\n",
    "The output layer only reads from the buffer.\n",
    "\n",
    "During backward, the order is reversed: the output layer stores its `dx` output into the last row, and next backward phases read their `dx` input from the next row  and store their `dx` output into their own row. The input layer only reads from the buffer.\n",
    "\n",
    "The image below illustrates the concept\n",
    "<figure>\n",
    "<img src=\"images/x_dx_buffers.png\"/>\n",
    "<figcaption> <b>Fig 6: </b> Connecting phases together: x and dx buffers\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db7a55",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0596b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from functools import partial\n",
    "from typing import Mapping, Optional\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from time import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import popxl\n",
    "from popxl import ReplicaGrouping\n",
    "import popxl_addons as addons\n",
    "import popxl.ops as ops\n",
    "from typing import Union\n",
    "from popxl_addons.named_tensors import NamedTensors\n",
    "from popxl_addons.named_replica_grouping import NamedReplicaGrouping\n",
    "from popxl_addons.variable_factory import NamedVariableFactories\n",
    "from popxl_addons import (\n",
    "    batch_serialise,\n",
    "    batch_serialise_fwd_and_grad,\n",
    "    batch_serial_buffer,\n",
    ")\n",
    "from popxl_addons.rts import (\n",
    "    all_gather_replica_sharded_graph,\n",
    "    replica_sharded_spec,\n",
    ")\n",
    "from popxl_addons.rts import reduce_replica_sharded_graph\n",
    "\n",
    "from popxl_addons.remote import (\n",
    "    NamedRemoteBuffers,\n",
    "    named_variable_buffers,\n",
    "    load_remote_graph,\n",
    "    store_remote_graph,\n",
    ")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37205615",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84a30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# includes gelu\n",
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True, gelu: bool = True, replica_grouping: Optional[ReplicaGrouping] = None):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.gelu = gelu\n",
    "        self.rg = replica_grouping\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # add a state variable to the module\n",
    "        w = self.add_variable_input(\n",
    "            \"weight\",\n",
    "            partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "            x.dtype,\n",
    "            replica_grouping=self.rg\n",
    "        )\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            # add a state variable to the module\n",
    "            b = self.add_variable_input(\"bias\", partial(np.zeros, y.shape[-1]), x.dtype, replica_grouping=self.rg)\n",
    "            y = y + b\n",
    "        if self.gelu:\n",
    "            y = ops.gelu(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class OutputLayerWithBwd(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True, gelu: bool = True, replica_grouping : Optional[ReplicaGrouping]=None):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(out_features=out_features, bias=bias, gelu=gelu, replica_grouping=replica_grouping)\n",
    "\n",
    "    def build(self, x: popxl.Tensor, labels=popxl.Tensor) -> popxl.Tensor:\n",
    "\n",
    "        fwd_facts, fwd_graph = self.linear.create_graph(x.spec)\n",
    "        bwd_facts, bwd_graph = addons.transforms.autodiff_with_accumulation(\n",
    "            fwd_graph,\n",
    "            tensors_to_accumulate_grads=fwd_graph.args.tensors,\n",
    "            grads_required=[fwd_graph.graph.inputs[0]],\n",
    "            replica_groupings=fwd_facts.replica_groupings\n",
    "        )\n",
    "\n",
    "        # outline forward\n",
    "        vars = self.add_variable_inputs(\"fwd\", fwd_facts)\n",
    "        fwd_info = fwd_graph.bind(vars).call_with_info(x)\n",
    "        x = fwd_info.parent_output(0)\n",
    "\n",
    "        loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "\n",
    "        # outline backward\n",
    "        bwd_vars = self.add_variable_inputs(\"bwd\", bwd_facts)\n",
    "        (dx,) = bwd_graph.bind(bwd_vars).call(\n",
    "            dx, args=bwd_graph.grad_graph_info.inputs_dict(fwd_info)\n",
    "        )\n",
    "\n",
    "        return dx, loss\n",
    "\n",
    "\n",
    "# gelu included in the linear layer\n",
    "class Net(addons.Module):\n",
    "    def __init__(self, rg: ReplicaGrouping, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        self.fc1 = Linear(512,rg)\n",
    "        self.fc2 = Linear(512,rg)\n",
    "        self.fc3 = Linear(512,rg)\n",
    "        self.fc4 = Linear(10, gelu=False)\n",
    "\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59743b",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af13a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adam optimizer.\n",
    "Defines adam update step for a single variable\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Adam(addons.Module):\n",
    "    # we need to specify in_sequence because a lot of operations are in place and their order\n",
    "    # shouldn't be rearranged\n",
    "    @popxl.in_sequence()\n",
    "    def build(\n",
    "        self,\n",
    "        var: popxl.TensorByRef,\n",
    "        grad: popxl.Tensor,\n",
    "        replica_grouping: Optional[popxl.ReplicaGrouping] = None,\n",
    "        *,\n",
    "        lr: Union[float, popxl.Tensor],\n",
    "        beta1: Union[float, popxl.Tensor] = 0.9,\n",
    "        beta2: Union[float, popxl.Tensor] = 0.999,\n",
    "        eps: Union[float, popxl.Tensor] = 1e-5,\n",
    "        weight_decay: Union[float, popxl.Tensor] = 0.0,\n",
    "        first_order_dtype: popxl.dtype = popxl.float16,\n",
    "        bias_correction: bool = True,\n",
    "    ):\n",
    "\n",
    "        # gradient estimators for the variable var - same shape as the variable\n",
    "\n",
    "        # Sharded inputs must be added with add_replica_sharded_variable_input\n",
    "        if var.meta_shape:\n",
    "            # shard over factor can be automatically computed from the variable\n",
    "            shard_over = np.prod(var.meta_shape) // np.prod(var.shape)\n",
    "            first_order = self.add_replica_sharded_variable_input(\n",
    "                \"first_order\",\n",
    "                partial(np.zeros, var.meta_shape),\n",
    "                first_order_dtype,\n",
    "                replica_grouping=replica_grouping,\n",
    "                shard_over=shard_over,\n",
    "                by_ref=True,\n",
    "            )\n",
    "            second_order = self.add_replica_sharded_variable_input(\n",
    "                \"second_order\",\n",
    "                partial(np.zeros, var.meta_shape),\n",
    "                popxl.float32,\n",
    "                replica_grouping=replica_grouping,\n",
    "                shard_over=shard_over,\n",
    "                by_ref=True,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            first_order = self.add_variable_input(\n",
    "                \"first_order\",\n",
    "                partial(np.zeros, var.shape),\n",
    "                first_order_dtype,\n",
    "                by_ref=True,\n",
    "                replica_grouping=replica_grouping\n",
    "            )\n",
    "            second_order = self.add_variable_input(\n",
    "                \"second_order\", partial(np.zeros, var.shape), popxl.float32, by_ref=True,\n",
    "                replica_grouping=replica_grouping\n",
    "            )\n",
    "\n",
    "        ops.var_updates.accumulate_moving_average_(first_order, grad, f=beta1)\n",
    "        ops.var_updates.accumulate_moving_average_square_(second_order, grad, f=beta2)\n",
    "\n",
    "        # adam is a biased estimator: provide the step to correct bias\n",
    "        step = None\n",
    "        if bias_correction:\n",
    "            step = self.add_variable_input(\n",
    "                \"step\", partial(np.zeros, ()), popxl.float32, by_ref=True\n",
    "            )\n",
    "\n",
    "        # calculate the weight increment with adam heuristic\n",
    "        updater = ops.var_updates.adam_updater(\n",
    "            first_order,\n",
    "            second_order,\n",
    "            weight=var,\n",
    "            weight_decay=weight_decay,\n",
    "            time_step=step,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            epsilon=eps,\n",
    "        )\n",
    "\n",
    "        # in place weight update: w += (-lr)*dw\n",
    "        ops.scaled_add_(var, updater, b=-lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ad3a2",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "692730ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Build the replica groupings to be used for replicated tensor sharding.\n",
    "If the tensor has less elements than the threshold, the group_size will be 1\n",
    "so that no sharding happens. Otherwise, tensors will be sharded across the data\n",
    "parallel replicas.\n",
    "\"\"\"\n",
    "\n",
    "def get_shard_groups(opts, facts: NamedVariableFactories) -> NamedReplicaGrouping:\n",
    "    ir = popxl.gcg().ir\n",
    "\n",
    "    rts_groups = {}\n",
    "    for k, f in facts.to_dict().items():\n",
    "        size = np.prod(f.shape)\n",
    "        rg = f.replica_grouping\n",
    "        if size >= opts.train.sharded_threshold and size % rg.group_size == 0:\n",
    "            rts_groups[k] = rg\n",
    "        else:\n",
    "            rts_groups[k] = ir.replica_grouping(group_size=1)\n",
    "    # it is important to sort the tensor names.\n",
    "    return dict(sorted(rts_groups.items()))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Groups together the forward, backward and optimizers graphs of a layer for easy access and handling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Graphs:\n",
    "    def __init__(\n",
    "        self,\n",
    "        opts,\n",
    "        layer: addons.Module,\n",
    "        optimizer: addons.Module,\n",
    "        entries: int,\n",
    "        require_dx_0: bool,\n",
    "        rg: ReplicaGrouping,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.rg = rg\n",
    "        # Create Graphs for computing forward, gradient and optimizer\n",
    "        fwd_facts, self.fwd = layer.create_graph(*args, **kwargs)\n",
    "        # variables and accumulators will be sharded according to the selected threshold\n",
    "        self.shard_groups = get_shard_groups(opts, fwd_facts)\n",
    "\n",
    "        required_grads = (self.fwd.graph.inputs[0],) if require_dx_0 else ()\n",
    "        grad_facts, self.bwd = addons.autodiff_with_accumulation(\n",
    "            self.fwd,\n",
    "            tensors_to_accumulate_grads=self.fwd.args.tensors,\n",
    "            grads_required=required_grads,\n",
    "        )\n",
    "        \n",
    "        optim_facts = self._setup_optim(optimizer, self.fwd.args, opts)\n",
    "        self._set_factories(fwd_facts, optim_facts, grad_facts)\n",
    "        self._setup_graphs(opts, entries)\n",
    "\n",
    "    @classmethod\n",
    "    def empty(cls):\n",
    "        return super().__new__(cls)\n",
    "\n",
    "    def from_fwd_and_bwd(\n",
    "        opts,\n",
    "        fwd_and_bwd: addons.Module,\n",
    "        optimizer: addons.Module,\n",
    "        entries: int,\n",
    "        rg: ReplicaGrouping,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        graphs = Graphs.empty()\n",
    "        graphs.bwd = None\n",
    "        graphs.rg = rg\n",
    "        facts, graphs.fwd = fwd_and_bwd.create_graph(*args, **kwargs)\n",
    "        graphs.shard_groups = get_shard_groups(opts, facts.fwd)\n",
    "        optim_facts = graphs._setup_optim(optimizer, graphs.fwd.args.fwd, opts)\n",
    "        graphs._set_factories(facts.fwd, optim_facts, facts.pop(\"bwd\"))\n",
    "        graphs._setup_graphs(opts, entries)\n",
    "        return graphs\n",
    "\n",
    "    def _setup_optim(self, optimizer: addons.Module, fwd_vars: NamedTensors, opts):\n",
    "        optim_facts = {}\n",
    "        self.optim = {}\n",
    "\n",
    "        for name, var in fwd_vars.to_dict().items():\n",
    "            optim_facts[name], self.optim[name] = optimizer.create_graph(\n",
    "                replica_sharded_spec(var, shard_over=self.shard_groups[name]),\n",
    "                replica_sharded_spec(var, shard_over=self.shard_groups[name]),\n",
    "                lr=opts.train.lr,\n",
    "                bias_correction=False,\n",
    "                replica_grouping=popxl.gcg().ir.replica_grouping(group_size=opts.train.data_parallel)\n",
    "            )\n",
    "        return optim_facts\n",
    "\n",
    "    def _set_factories(self, fwd_facts, optim_facts, grad_facts):\n",
    "        self.facts = NamedVariableFactories(\n",
    "            fwd=fwd_facts, optim=NamedVariableFactories.from_dict(optim_facts)\n",
    "        )\n",
    "        self.grad_facts = grad_facts\n",
    "\n",
    "    def _setup_graphs(self, opts, entries: int):\n",
    "        # Create remote buffers for fwd vars and optimiser state.\n",
    "\n",
    "        # only require the group size\n",
    "        shard_dict = { n : g.group_size for n,g in self.shard_groups.items() }\n",
    "        optim_shard_dict = {n: g.group_size for n, g in get_shard_groups(opts,self.facts.optim).items()}\n",
    "\n",
    "        self.buffers = NamedRemoteBuffers(\n",
    "            fwd=named_variable_buffers(self.facts.fwd, entries, shard_over_dict=shard_dict),\n",
    "            optim=named_variable_buffers(self.facts.optim, entries, shard_over_dict=optim_shard_dict)\n",
    "        )\n",
    "        # Create Graphs for loading/gathering/storing/reducing\n",
    "        self._fwd_load, self._fwd_load_names = load_remote_graph(\n",
    "            self.buffers.fwd, entries\n",
    "        )\n",
    "        self._optim_fwd_load, self._optim_fwd_load_names = load_remote_graph(\n",
    "            self.buffers, entries\n",
    "        )\n",
    "        self._optim_fwd_store = store_remote_graph(self.buffers, entries)\n",
    "        (self._fwd_all_gather, self._fwd_all_gather_names,) = all_gather_replica_sharded_graph(\n",
    "            NamedTensors.pack(self._fwd_load_names, self._fwd_load.graph.outputs),\n",
    "            replica_groups=NamedReplicaGrouping.from_dict(dict(zip(self._fwd_load_names, self.shard_groups.values())))\n",
    "        )\n",
    "        grad_accums = self.bwd.args.copy() if self.bwd else self.fwd.args.bwd.copy()\n",
    "        grad_accums.pop(\"mean_accum_counter\")\n",
    "        self._grad_reduce, self._grad_reduce_names = reduce_replica_sharded_graph(\n",
    "            grad_accums, \"mean\", shard_groups=NamedReplicaGrouping.from_dict(get_shard_groups(opts,self.grad_facts)), replica_group=self.rg\n",
    "        )\n",
    "\n",
    "    # load forward variables\n",
    "    def fwd_load(self, i: Union[int, popxl.Tensor]):\n",
    "        return NamedTensors.pack(self._fwd_load_names, self._fwd_load.call(i))\n",
    "\n",
    "    # load forward variables and optimizer state\n",
    "    def optim_fwd_load(self, i: Union[int, popxl.Tensor]):\n",
    "        return NamedTensors.pack(\n",
    "            self._optim_fwd_load_names, self._optim_fwd_load.call(i)\n",
    "        )\n",
    "\n",
    "    # store forward variables and optimizer state\n",
    "    def optim_fwd_store(self, args: NamedTensors, i: Union[int, popxl.Tensor]):\n",
    "        return self._optim_fwd_store.bind(args).call(i)\n",
    "\n",
    "    # gathers replica sharded forward variables\n",
    "    def fwd_all_gather(self, args: NamedTensors):\n",
    "        return NamedTensors.pack(\n",
    "            self._fwd_all_gather_names, self._fwd_all_gather.bind(args).call()\n",
    "        )\n",
    "\n",
    "    # reduce scatter gradients\n",
    "    def grad_reduce(self, args: NamedTensors):\n",
    "        return NamedTensors.pack(\n",
    "            self._grad_reduce_names, self._grad_reduce.bind(args).call()\n",
    "        )\n",
    "\n",
    "    # update forward variables\n",
    "    def optimizer_remote_step(\n",
    "        self,\n",
    "        i: int,\n",
    "        vars_and_state: NamedTensors,\n",
    "        grads: NamedTensors,\n",
    "        accum_counter: popxl.Tensor,\n",
    "    ):\n",
    "        _variables = vars_and_state.fwd.to_dict()\n",
    "        _state = vars_and_state.optim\n",
    "        _grads = grads.accum.to_dict()\n",
    "        for name, graph in self.optim.items():\n",
    "            state_clean_names = self._get_optimizer_state(name, _state)\n",
    "            self.optim[name].bind(state_clean_names).call(\n",
    "                _variables[name], _grads[name]\n",
    "            )\n",
    "        ops.var_updates.accumulator_scale_(accum_counter, 0.0)\n",
    "\n",
    "    def _get_optimizer_state(self, name: str, state: NamedTensors) -> NamedTensors:\n",
    "        attrs = name.split(\".\")\n",
    "        for attr in attrs:\n",
    "            state = getattr(state, attr)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13512b9d",
   "metadata": {},
   "source": [
    "### Batch serialisation utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6ae7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_layer_batch_serialise(\n",
    "    opts,\n",
    "    layer_graphs: Graphs,\n",
    "    x_buffer: popxl.RemoteBuffer,\n",
    "    dx_buffer: popxl.RemoteBuffer,\n",
    "    input_stream: popxl.HostToDeviceStream,\n",
    "):\n",
    "    fwd_bs, bwd_bs = batch_serialise_fwd_and_grad(\n",
    "        layer_graphs.fwd,\n",
    "        layer_graphs.bwd,\n",
    "        layer_graphs.fwd.args,\n",
    "        opts.train.gradient_accumulation,\n",
    "        load_handles={\n",
    "            layer_graphs.fwd.graph.inputs[0]: input_stream,\n",
    "            layer_graphs.bwd.graph.inputs[0]: (dx_buffer, 0),\n",
    "        },\n",
    "        store_streams={},\n",
    "        store_buffers={\n",
    "            layer_graphs.fwd.graph.outputs[0]: (x_buffer, 0),\n",
    "        },\n",
    "        rows=1,\n",
    "        io_mode=opts.train.io_mode,\n",
    "    )\n",
    "    layer_graphs.fwd = fwd_bs.graph\n",
    "    layer_graphs.bwd = bwd_bs.graph\n",
    "\n",
    "\n",
    "def layer_batch_serialise(\n",
    "    opts,\n",
    "    layer_graphs: Graphs,\n",
    "    x_buffer: popxl.RemoteBuffer,\n",
    "    dx_buffer: popxl.RemoteBuffer,\n",
    "    rows: int,\n",
    "):\n",
    "    fwd_bs, bwd_bs = batch_serialise_fwd_and_grad(\n",
    "        layer_graphs.fwd,\n",
    "        layer_graphs.bwd,\n",
    "        layer_graphs.fwd.args,\n",
    "        opts.train.gradient_accumulation,\n",
    "        load_handles={\n",
    "            layer_graphs.fwd.graph.inputs[0]: (\n",
    "                x_buffer,\n",
    "                0,\n",
    "            ),  # load x from previous layer row\n",
    "            layer_graphs.bwd.graph.inputs[0]: (\n",
    "                dx_buffer,\n",
    "                1,\n",
    "            ),  # load dx from next layer row\n",
    "        },\n",
    "        store_streams={},\n",
    "        store_buffers={\n",
    "            layer_graphs.fwd.graph.outputs[0]: (\n",
    "                x_buffer,\n",
    "                1,\n",
    "            ),  # store x in next layer row\n",
    "            layer_graphs.bwd.graph.outputs[0]: (\n",
    "                dx_buffer,\n",
    "                0,\n",
    "            ),  # store dx in previous layer row\n",
    "        },\n",
    "        rows=2,\n",
    "        io_mode=opts.train.io_mode,\n",
    "    )\n",
    "    layer_graphs.fwd = fwd_bs.graph\n",
    "    layer_graphs.bwd = bwd_bs.graph\n",
    "\n",
    "\n",
    "def output_layer_batch_serialise(\n",
    "    opts,\n",
    "    layer_graphs: Graphs,\n",
    "    x_buffer: popxl.RemoteBuffer,\n",
    "    dx_buffer: popxl.RemoteBuffer,\n",
    "    label_stream: popxl.h2d_stream,\n",
    "    output_stream: popxl.d2h_stream,\n",
    "):\n",
    "    fwd_bs = batch_serialise(\n",
    "        layer_graphs.fwd,\n",
    "        opts.train.gradient_accumulation,\n",
    "        load_handles={\n",
    "            layer_graphs.fwd.graph.inputs[0]: (x_buffer, 2),\n",
    "            layer_graphs.fwd.graph.inputs[1]: label_stream,\n",
    "        },\n",
    "        store_streams={layer_graphs.fwd.graph.outputs[1]: output_stream},\n",
    "        store_buffers={layer_graphs.fwd.graph.outputs[0]: (dx_buffer, 2)},\n",
    "        rows=1,\n",
    "        io_mode=opts.train.io_mode,\n",
    "    )\n",
    "    layer_graphs.fwd = fwd_bs.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93691ccc",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a393c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_throughput(session, samples_per_step, epochs: int = 5):\n",
    "    inputs = {\n",
    "        stream: np.ones(\n",
    "            session._full_input_shape(stream.shape), stream.dtype.as_numpy()\n",
    "        )\n",
    "        for stream in session.expected_inputs()\n",
    "    }\n",
    "\n",
    "    durations = []\n",
    "    with session:\n",
    "        for i in range(epochs):\n",
    "            start = time()\n",
    "            session.run(inputs)\n",
    "            dur = time() - start\n",
    "            durations.append(dur)\n",
    "\n",
    "    duration = np.mean(durations)\n",
    "\n",
    "    result_str = (\n",
    "        f\"Mean duration: {duration} s \"\n",
    "        f\"Throughput: {samples_per_step/duration:6.1f} samples/s \"\n",
    "    )\n",
    "    print(result_str)\n",
    "\n",
    "\n",
    "def get_mnist_data(test_batch_size: int, batch_size: int):\n",
    "    training_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(\n",
    "                        (0.1307,), (0.3081,)\n",
    "                    ),  # mean and std computed on the training set.\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    validation_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def accuracy(predictions: np.ndarray, labels: np.ndarray):\n",
    "    ind = np.argmax(predictions, axis=-1).flatten()\n",
    "    labels = labels.detach().numpy().flatten()\n",
    "    return np.mean(ind == labels) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e7bf7",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04660ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Train:\n",
    "    micro_batch_size: int = 8\n",
    "    lr: Union[float, popxl.Tensor] = 1e-3\n",
    "    epochs: int = 1\n",
    "    gradient_accumulation: int = 1\n",
    "    data_parallel: int = 1\n",
    "    device = \"ipu_hw\"\n",
    "    sharded_threshold: int = 512\n",
    "    io_mode: str = \"io\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Test:\n",
    "    micro_batch_size: int = 80\n",
    "    device = \"ipu_hw\"\n",
    "\n",
    "\n",
    "class Options:\n",
    "    train = Train()\n",
    "    test = Test()\n",
    "\n",
    "\n",
    "opts = Options()\n",
    "opts.train.micro_batch_size = 8\n",
    "opts.train.lr = 1e-3\n",
    "opts.train.epochs = 1\n",
    "opts.train.gradient_accumulation = 4\n",
    "opts.train.data_parallel = 2\n",
    "opts.train.sharded_threshold = 512\n",
    "opts.train.io_mod = \"io\"\n",
    "\n",
    "opts.test.micro_batch_size = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c9c2ef",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87572ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_program(opts):\n",
    "    assert opts.train.gradient_accumulation > 1\n",
    "    assert opts.train.data_parallel > 1\n",
    "\n",
    "    ir = popxl.Ir()\n",
    "    ir.replication_factor = opts.train.data_parallel\n",
    "    num_inner_layers = 2\n",
    "    rg = ir.replica_grouping(group_size=opts.train.data_parallel)\n",
    "    if opts.train.io_mode != \"compute\":\n",
    "        session_opts = ir._pb_ir.getSessionOptions()\n",
    "        session_opts.numIOTiles = 32\n",
    "\n",
    "    with ir.main_graph:\n",
    "        # -----  Define input and output streams -----\n",
    "        img_spec = popxl.TensorSpec(\n",
    "            (opts.train.micro_batch_size, 28 * 28), popxl.float32\n",
    "        )\n",
    "        inner_spec = popxl.TensorSpec((opts.train.micro_batch_size, 512), popxl.float32)\n",
    "\n",
    "        img_stream = popxl.h2d_stream(img_spec.shape, popxl.float32, \"image\")\n",
    "        label_stream = popxl.h2d_stream(\n",
    "            (opts.train.micro_batch_size,), popxl.int32, \"labels\"\n",
    "        )\n",
    "        loss_stream = popxl.d2h_stream((), popxl.float32, \"loss\")\n",
    "        optimizer = Adam(cache=False)\n",
    "\n",
    "        # ----- Create graphs -----\n",
    "        fc1 = Graphs(opts, Linear(512, replica_grouping=rg), optimizer, 1, False, rg, img_spec)\n",
    "\n",
    "        inner_layer = Graphs(\n",
    "            opts, Linear(512, replica_grouping=rg), optimizer, num_inner_layers, True, rg, inner_spec\n",
    "        )\n",
    "\n",
    "        fc4_fwd_bwd = Graphs.from_fwd_and_bwd(\n",
    "            opts,\n",
    "            OutputLayerWithBwd(10, gelu=False, replica_grouping=rg),\n",
    "            optimizer,\n",
    "            1,\n",
    "            rg,\n",
    "            inner_spec,\n",
    "            label_stream.spec,\n",
    "        )\n",
    "\n",
    "        x_buffer = batch_serial_buffer(\n",
    "            fc1.fwd.graph.outputs[0],\n",
    "            steps=opts.train.gradient_accumulation,\n",
    "            rows=num_inner_layers + 1,\n",
    "        )\n",
    "        dx_buffer = batch_serial_buffer(\n",
    "            fc1.bwd.graph.inputs[0],\n",
    "            steps=opts.train.gradient_accumulation,\n",
    "            rows=num_inner_layers + 1,\n",
    "        )\n",
    "\n",
    "        # ----- Transform graphs -----\n",
    "\n",
    "        # apply batch serialisation\n",
    "        input_layer_batch_serialise(opts, fc1, x_buffer, dx_buffer, img_stream)\n",
    "        layer_batch_serialise(\n",
    "            opts, inner_layer, x_buffer, dx_buffer, num_inner_layers\n",
    "        )  # use a buffer with two rows because the inner layer is duplicated two times. row 0 is for fc2 and row 1 for fc3\n",
    "        output_layer_batch_serialise(\n",
    "            opts, fc4_fwd_bwd, x_buffer, dx_buffer, label_stream, loss_stream\n",
    "        )\n",
    "\n",
    "        # ----- Create Variables -----\n",
    "        variables = NamedTensors()\n",
    "        variables.insert(\"fc1\", fc1.facts.init_remote(fc1.buffers, 0, \"fc1\"))\n",
    "        variables.insert(\n",
    "            \"fc2\", inner_layer.facts.init_remote(inner_layer.buffers, 0, \"fc2\")\n",
    "        )\n",
    "        variables.insert(\n",
    "            \"fc3\", inner_layer.facts.init_remote(inner_layer.buffers, 1, \"fc3\")\n",
    "        )\n",
    "        variables.insert(\n",
    "            \"fc4\", fc4_fwd_bwd.facts.init_remote(fc4_fwd_bwd.buffers, 0, \"fc4\")\n",
    "        )\n",
    "\n",
    "        # ----- Construct Execution Scheme -----\n",
    "\n",
    "        # Phased Execution (with remote fwd variables and optimizer state). N layers executing separately\n",
    "        # phase 1: fwd for layer 1:\n",
    "        #   load fwd variables.\n",
    "        #   for i in range(gradient_accumulation_steps):\n",
    "        #        load inputs (xs)\n",
    "        #        execute fwd\n",
    "        #        store outputs & activations\n",
    "        #\n",
    "        # phase 2: fwd for layer 2:\n",
    "        # ...\n",
    "        # phase N: fwd + bwd + optimizer for layer N:\n",
    "        #   load fwd variables and optimizer state.\n",
    "        #   for i in range(gradient_accumulation_steps):\n",
    "        #       load fwd and bwd inputs (xs)\n",
    "        #       execute fwd\n",
    "        #       compute loss\n",
    "        #       execute bwd\n",
    "        #       store outputs & activations\n",
    "        #   call optimizer\n",
    "        #   store updated fwd variables and optimizer state\n",
    "        #\n",
    "        # phase N+1: bwd for layer N-1 + optimizer\n",
    "        #   load fwd variables and optimizer state. both needed, fwd vars are needed from the backward.\n",
    "        #   for i in range(gradient_accumulation_steps):\n",
    "        #       load bwd inputs (xs)\n",
    "        #       execute bwd\n",
    "        #       store outputs & activations\n",
    "        #   call optimizer\n",
    "        #   store updated fwd variables and optimizer state\n",
    "        # ...\n",
    "        # phase 2N-1: bwd for layer 1 + optimizer\n",
    "        #\n",
    "\n",
    "        # ----- Phased Execution -----\n",
    "        with popxl.in_sequence(True):\n",
    "\n",
    "            def forward_phase(graphs: Graphs, row_offset: int):\n",
    "                vars = graphs.fwd_load(\n",
    "                    row_offset\n",
    "                )  # load forward remote variables, which are sharded\n",
    "                vars = graphs.fwd_all_gather(\n",
    "                    vars\n",
    "                )  # gathered variables: graph must be bound to gathered vars\n",
    "                # calling the graph executes the GA loop for the phase: repeat ( load xs - execute - store )\n",
    "                graphs.fwd.bind(vars).call(row_offset)\n",
    "\n",
    "            def backward_phase(graphs: Graphs, row_offset: int):\n",
    "                is_joint_fwd_bwd = graphs.bwd is None\n",
    "                # forward vars and optimizer state are needed in the backward.\n",
    "                # loading them together is convenient\n",
    "                fwd_vars_and_state = graphs.optim_fwd_load(row_offset)  # sharded\n",
    "                vars: NamedTensors  # gathered variables comprising forward and backward named inputs\n",
    "                reduced_grads: NamedTensors  # scattered gradient accumulators\n",
    "                mean_accum_counter: popxl.Tensor\n",
    "                if is_joint_fwd_bwd:\n",
    "                    vars = NamedTensors(\n",
    "                        fwd=graphs.fwd_all_gather(\n",
    "                            fwd_vars_and_state.fwd\n",
    "                        ),  # gathered forward variables\n",
    "                        bwd=graphs.grad_facts.init_zero(),  # gradient accumulators\n",
    "                    )\n",
    "                    # calling the graph executes the GA loop for the phase: repeat ( load xs - execute fwd compute loss execute bwd - store )\n",
    "                    graphs.fwd.bind(vars).call(\n",
    "                        row_offset\n",
    "                    )  # the fwd graph includes everything\n",
    "                    reduced_grads = graphs.grad_reduce(vars.bwd)\n",
    "                    mean_accum_counter = vars.bwd.mean_accum_counter\n",
    "                else:\n",
    "                    vars = graphs.fwd_all_gather(fwd_vars_and_state.fwd)\n",
    "                    grad_accums = graphs.grad_facts.init_zero()  # gradient accumulators\n",
    "                    vars.update(grad_accums.copy())\n",
    "                    # calling the graph executes the GA loop for the phase: repeat ( load xs - execute bwd - store )\n",
    "                    graphs.bwd.bind(vars).call(\n",
    "                        row_offset\n",
    "                    )  # just call the batch serialised bwd\n",
    "                    reduced_grads = graphs.grad_reduce(grad_accums)\n",
    "                    mean_accum_counter = vars.mean_accum_counter\n",
    "                # optimizer\n",
    "                graphs.optimizer_remote_step(\n",
    "                    row_offset, fwd_vars_and_state, reduced_grads, mean_accum_counter\n",
    "                )\n",
    "                graphs.optim_fwd_store(\n",
    "                    fwd_vars_and_state, row_offset\n",
    "                )  # store updated vars\n",
    "\n",
    "            # ----- Phase 1 (fwd): fc1 Fwd -----\n",
    "            forward_phase(fc1, 0)\n",
    "            # ----- Phase 2 (fwd): fc2 Fwd-----\n",
    "            forward_phase(inner_layer, 0)\n",
    "            # ----- Phase 3 (fwd): fc2 Fwd-----\n",
    "            forward_phase(inner_layer, 1)\n",
    "            # ----- Phase 4 (merged fwd-bwd): Fwd for output layer, loss,  Bwd for output layer - Optimizer for output layer -----\n",
    "            backward_phase(fc4_fwd_bwd, 0)\n",
    "            # ----- Phase 5 (bwd): fc3 bwd - Optimizer for fc3 -----\n",
    "            backward_phase(inner_layer, 1)\n",
    "            # ----- Phase 6 (bwd): fc2 bwd - Optimizer for fc2 -----\n",
    "            backward_phase(inner_layer, 0)\n",
    "            # ----- Phase 7 (bwd): fc1 bwd - Optimizer for fc1 -----\n",
    "            backward_phase(fc1, 0)\n",
    "\n",
    "    # we have a for loop, the number of host loads is equal to gradient_accumulation\n",
    "    ir.num_host_transfers = opts.train.gradient_accumulation\n",
    "    # weights we need to retrieve and copy to the test session. They need to be in the same names as the full model (fc1-fc2-fc3-fc4).\n",
    "    vars = NamedTensors(\n",
    "        fc1=variables.fc1.fwd,\n",
    "        fc2=variables.fc2.fwd,\n",
    "        fc3=variables.fc3.fwd,\n",
    "        fc4=variables.fc4.fwd,\n",
    "    )\n",
    "\n",
    "    return popxl.Session(ir, \"ipu_hw\"), [img_stream, label_stream], vars, loss_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "419eb85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The compile time engine option debug.branchRecordTile is set to \"2943\" when creating the Engine. (At compile-tile it was set to 1471)\n"
     ]
    }
   ],
   "source": [
    "global_batch_size = (\n",
    "    opts.train.micro_batch_size\n",
    "    * opts.train.gradient_accumulation\n",
    "    * opts.train.data_parallel\n",
    ")\n",
    "training_data, test_data = get_mnist_data(opts.test.micro_batch_size, global_batch_size)\n",
    "train_session, train_input_streams, train_variables, loss_stream = train_program(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bd1db99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss:0.1070: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 937/937 [00:12<00:00, 72.15it/s]\n"
     ]
    }
   ],
   "source": [
    "nr_batches = len(training_data)\n",
    "for epoch in range(1, opts.train.epochs + 1):\n",
    "    nr_batches = len(training_data)\n",
    "    with train_session:\n",
    "        for epoch in range(1, opts.train.epochs + 1):\n",
    "            print(\"Epoch {0}/{1}\".format(opts.train.epochs, opts.train.epochs))\n",
    "            bar = tqdm(training_data, total=nr_batches)\n",
    "            for data, labels in bar:\n",
    "                # reshape data accounting for replication and num hosts transfers\n",
    "                data = data.reshape(\n",
    "                    train_session.ir.num_host_transfers,\n",
    "                    train_session.ir.replication_factor,\n",
    "                    opts.train.micro_batch_size,\n",
    "                    28 * 28,\n",
    "                ).squeeze()\n",
    "                labels = labels.reshape(\n",
    "                    train_session.ir.num_host_transfers,\n",
    "                    train_session.ir.replication_factor,\n",
    "                    opts.train.micro_batch_size,\n",
    "                ).squeeze()\n",
    "\n",
    "                inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "                    zip(train_input_streams, [data.squeeze().float(), labels.int()])\n",
    "                )\n",
    "                loss = train_session.run(inputs)\n",
    "                losses_np = loss[\n",
    "                    loss_stream\n",
    "                ]  # shape(ir.num_host_transfers, ir.replication_factor, )\n",
    "                avg_loss = np.mean(losses_np)\n",
    "                bar.set_description(\"Loss:{:0.4f}\".format(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bad0ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights data : dictionary { train_session variables : tensor data (numpy) }\n",
    "train_vars_to_data = train_session.get_tensors_data(train_variables.tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68a62f",
   "metadata": {},
   "source": [
    "### Throughput and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d9b48a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_program(test_batch_size, device):\n",
    "    ir = popxl.Ir(replication=1)\n",
    "    with ir.main_graph:\n",
    "        # Inputs\n",
    "        in_stream = popxl.h2d_stream((test_batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        in_t = ops.host_load(in_stream)\n",
    "\n",
    "        # Create graphs\n",
    "        rg = ir.replica_grouping(group_size=1)\n",
    "        facts, graph = Net(rg).create_graph(in_t)\n",
    "\n",
    "        # Initialise variables\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Forward\n",
    "        (outputs,) = graph.bind(variables).call(in_t)\n",
    "        out_stream = popxl.d2h_stream(outputs.shape, outputs.dtype, \"outputs\")\n",
    "        ops.host_store(out_stream, outputs)\n",
    "\n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [in_stream], variables, out_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44f52c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train session\n",
      "Mean duration: 0.003766155242919922 s Throughput: 16993.5 samples/s \n"
     ]
    }
   ],
   "source": [
    "# Create test program and test session\n",
    "test_session, test_input_streams, test_variables, out_stream = test_program(\n",
    "    opts.test.micro_batch_size, opts.test.device\n",
    ")\n",
    "\n",
    "# dictionary { train_session variables : test_session variables }\n",
    "train_vars_to_test_vars = train_variables.to_mapping(test_variables)\n",
    "\n",
    "# Create a dictionary { test_session variables : tensor data (numpy) }\n",
    "# We want to copy the values before evaluating throughput on synthetic data, otherwise weights are changed\n",
    "test_vars_to_data = {\n",
    "    test_var: train_vars_to_data[train_var].copy()\n",
    "    for train_var, test_var in train_vars_to_test_vars.items()\n",
    "}\n",
    "\n",
    "# Copy trained weights to the program, with a single host to device transfer at the end\n",
    "test_session.write_variables_data(test_vars_to_data)\n",
    "\n",
    "# evaluate the ratio samples per step / time for train session\n",
    "print(\"train session\")\n",
    "evaluate_throughput(train_session, global_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "717e0d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:01<00:00, 92.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 96.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nr_batches = len(test_data)\n",
    "sum_acc = 0.0\n",
    "with test_session:\n",
    "    for data, labels in tqdm(test_data, total=nr_batches):\n",
    "        inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "            zip(test_input_streams, [data.squeeze().float(), labels.int()])\n",
    "        )\n",
    "        output = test_session.run(inputs)\n",
    "        sum_acc += accuracy(output[out_stream], labels)\n",
    "print(\"Accuracy on test set: {:0.2f}%\".format(sum_acc / len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6269283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test session\n",
      "Mean duration: 0.0008016109466552734 s Throughput: 99799.0 samples/s \n"
     ]
    }
   ],
   "source": [
    "samples_per_step = (\n",
    "    opts.test.micro_batch_size\n",
    ")  # no data parallelism or gradient accumulation for inference in this program\n",
    "# evaluate the ratio samples per step / time for train session\n",
    "print(\"test session\")\n",
    "evaluate_throughput(test_session, samples_per_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
