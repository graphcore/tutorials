{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2022 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining\n",
    "\n",
    "Up to now, we've run the entire model on a single IPU. This requires\n",
    "the variables (and other tensors) of all layers to fit within the IPU's memory. However, large models may exceed the memory capacity on a single IPU.\n",
    "\n",
    "To train such models while maintaining high IPU utilisation we can use  [**Pipelining**](https://docs.graphcore.ai/projects/ipu-programmers-guide/en/3.1.0/algorithmic_techniques.html?highlight=micro%20batch#model-parallelism-and-pipelining): we first split the model into **independent parts** ( pipeline **stages** ) and then setup the program so that they can be **executed by different IPUs in parallel** (thus, it is a form of **model parallelism**).\n",
    "\n",
    "## Efficient parallel execution with pipelining\n",
    "\n",
    "Suppose we have a model made of three layers and we define five pipeline stages:\n",
    "- layer1 forward on IPU0\n",
    "- layer2 forward on IPU1\n",
    "- layer3 forward, loss computation, layer3 backward on IPU3\n",
    "- layer2 backward on IPU1\n",
    "- layer1 backward on IPU0\n",
    "\n",
    "If only a batch is processed at each time, the pipeline is very inefficient, because we have a data dependency between the stages and they can't execute in parallel: only one ipu will be active.\n",
    "\n",
    "- layer1 forward on IPU0\n",
    "- layer2 forward on IPU1 **data dependency: needs x = layer1(x)**\n",
    "- layer3 forward, loss computation, layer3 backward on IPU3 **data dependency: needs x = layer2(x)**\n",
    "- layer2 backward on IPU1 **data dependency: needs dx = dlayer3(dx,activations3)**\n",
    "- layer1 backward on IPU0 **data dependency: needs dx = dlayer2(dx,activations2)**\n",
    "\n",
    "![Figure 1: Inefficient pipelining](images/inefficient_pipeline.png)\n",
    "<figcaption> <b>Fig 1: </b> Inefficient pipelining: a single batch B<sub>i</sub> is processed. There is a data dependency hence only one IPU can be active to compute activations A<sub>i</sub> and gradients G<sub>i</sub>.\n",
    " </figcaption>\n",
    "\n",
    "To implement pipelining efficiently, we need to **break the data dependency** by loading multiple micro batches and having each pipeline stage running in parallel on a different micro batch.\n",
    "Gradient accumulation is perfect here, since we also need to keep the weights of the layers frozen during the whole pipeline execution and update them only at the end. \n",
    "\n",
    "![Figure 2: Efficient pipelining](images/pipelining_parallel.png)\n",
    " <figcaption> <b>Fig 2: </b> Efficient pipelining: multiple micro batches are loaded using gradient accumulation. Pipeline stages can execute in parallel <b>on different micro batches</b> and all IPUs are used in the main phase when the pipeline is full ( while in the ramp up and ramp down phase only a few are active)\n",
    " </figcaption>\n",
    "\n",
    "### Activation Stash and Recomputation\n",
    "Note that the gradient computation for a layer requires the layers activations calculated with respect to the same batch. In a pipeline, this computation is separated in time: when the IPU is running the backward for the layer, it has already run `backward_stage - forward_stage` forward steps. Hence we need to save all the forward activations in a FIFO queue of length `backward_stage - forward_stage`, the **activation stash**. This can be inconvenient from a memory perspective. We can instead choose to recompute the activations (all or some) to save memory at the expense of extra computation.\n",
    "See also the [memory stashes and recomputation](https://docs.graphcore.ai/projects/ipu-programmers-guide/en/3.1.0/algorithmic_techniques.html?highlight=micro%20batch#memory-stashes-and-recomputation) paragraph in the ipu programmer guide.\n",
    "\n",
    "### Efficient pipelining best practices\n",
    "\n",
    "- With pipelining, the **`gradient_accumulation`** factor needs to be greater than **2 * number of pipeline stages** (Fig 2 shows it clearly). The greater, the better the IPUs utilisation since in that case ramp up and ramp down phases, when not all IPUs are active, only take a small fraction of the execution.\n",
    "- To get the best utilisation, the layers across the IPUs should be balanced such that they each take roughly the same amount of time to compute. \n",
    "- Forward and backward stages for the same layers should be defined on the same IPU, since they require the same parameters. Moreover, backward requires the layer activations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelining in addons\n",
    "\n",
    "Consider the loop:\n",
    "\n",
    "```python\n",
    "for i in range(N):\n",
    "  x = ops.host_load(h2d_stream)\n",
    "  x = layer0.call(x) # we want this to be our stage0 on ipu0\n",
    "  x = x.copy_to_ipu(1)\n",
    "  x = layer1.call(x) # we want this to be our stage1 on ipu1\n",
    "  ops.host_store(d2h_stream, x)\n",
    "```\n",
    "There is a data dependency between `layer0` -> `layer1`. As explained in the previous section, if we rewrite the program so that `layer0` and `layer1` run on different data, we break the dependency and we are able to execute them in parallel.\n",
    "\n",
    "```python\n",
    "# ----------- RAMP UP PHASE - only ipu0 active -----------\n",
    "#load first batch, batch0\n",
    "x0 = ops.host_load(h2d_stream) \n",
    "# run layer0 on batch0\n",
    "x0_ = layer0(x0) \n",
    "# copy the output of layer0 to ipu1, where layer1 lives. \n",
    "x1 = x0_.copy_to_ipu(1)\n",
    "\n",
    "# ------------- MAIN PHASE: both ipus active --------------\n",
    "for i in range(N-1):\n",
    "    # load next batch: x0 = batch1\n",
    "    x0 = ops.host_load(h2d_stream) \n",
    "    # layer0 can run on batch1\n",
    "    x0_ = layer0(x0) \n",
    "    # while layer1 runs on batch0\n",
    "    x1_ = layer1(x1)\n",
    "    ops.host_store(d2h_stream, x1_)\n",
    "    # copy the output of layer0 to ipu1 for next iteration\n",
    "    x1 = x0_.copy_to_ipu(1)\n",
    "    \n",
    "# ----------- RAMP DOWN PHASE - only ipu1 active -----------\n",
    "x1_ = layer1(x1)\n",
    "ops.host_store(d2h_stream, x1_)\n",
    "```\n",
    "\n",
    "The `addons.pipeline_transform` will transform the first loop into the second one. It adds ramp-up and ramp-down phases and reorders and merges synchronisation, eliminating data dependencies so that stages can be executed in parallel.\n",
    "\n",
    "\n",
    "### The pipeline transform \n",
    "Before implementing a pipeline you should outline the model and create the graphs corresponding to each layer on the ipu you want them to be executed on. You can use `popxl.ipu(ipu_index)` context for that.\n",
    "Every operation creation must take place in a `ipu` context.\n",
    "\n",
    "Once you have your layers, you can define a pipeline.\n",
    "To define a pipeline in addons you need to write your code in the `with addons.pipelined_execution(steps=N) as pipeline` context.\n",
    "This defines the loop `for i in range(N)` to be transformed. \n",
    "\n",
    "Within this context you specify the pipeline stages writing operations inside a `pipeline.stage(stage_index)` context and you decide the ipu where they are executed using `popxl.ipu(ipu_index)` context.\n",
    "The code snippet below shows a basic pipeline for a two-layers forward.\n",
    "\n",
    "When the pipeline context closes, the transformation is run and the current graph ends up with a single `ops.call` that executes the pipeline:\n",
    "\n",
    "There are no constraints on how many or which ipus a stage can run on. However, any communication between ipus will cause a syncronisation that can stall the pipeline. In general, we want the data dependencies between stages to only be represented as `ops.ipu_copy` or `Tensor.copy_to_ipu`.\n",
    "\n",
    "Being a loop, if you have `host_load` in the pipeline you need to remember to set the `ir.num_host_transfers` property equal to the pipeline steps.\n",
    "\n",
    "Example below shows a basic pipeline for a 2-layer forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import popxl\n",
    "import popxl_addons as addons\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "class Add(addons.Module):\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        w = self.add_variable_input(\n",
    "            \"weight\", partial(np.random.normal, 0, 0.02, x.shape), x.dtype\n",
    "        )\n",
    "        x = popxl.ops.add(w, x)\n",
    "        return x\n",
    "\n",
    "\n",
    "ir = popxl.Ir(replication=1)\n",
    "N = 4\n",
    "\n",
    "with ir.main_graph:\n",
    "    # inputs\n",
    "    input_stream = popxl.h2d_stream((32,), dtype=popxl.float32)\n",
    "\n",
    "    # create graphs in the appropriate ipu context\n",
    "    with popxl.ipu(0):\n",
    "        facts0, layer0 = Add().create_graph(input_stream.spec)\n",
    "\n",
    "    with popxl.ipu(1):\n",
    "        facts1, layer1 = Add().create_graph(input_stream.spec)\n",
    "\n",
    "    # transform the graph if needed\n",
    "\n",
    "    # bind the graphs when you prefer\n",
    "    bound_layer0 = layer0.bind(facts0.init())\n",
    "    bound_layer1 = layer1.bind(facts1.init())\n",
    "\n",
    "    with popxl.in_sequence(True):\n",
    "        # pipeline context\n",
    "        with addons.pipelined_execution(steps=N) as p:\n",
    "            # define stages on the appropriate ipu\n",
    "            with p.stage(0), popxl.ipu(0):\n",
    "                x = popxl.ops.host_load(input_stream)\n",
    "                (x,) = bound_layer0.call(x)\n",
    "                x = x.copy_to_ipu(1)\n",
    "\n",
    "            with p.stage(1), popxl.ipu(1):\n",
    "                x = bound_layer1.call(x)\n",
    "\n",
    "graph = addons.GraphWithNamedArgs(ir.main_graph)\n",
    "print(\"The current graph only has a call to the pipeline subgraph \\n\")\n",
    "print(graph.print_schedule())\n",
    "ir.num_host_transfers = N\n",
    "inputs = np.ones(\n",
    "    (\n",
    "        N,\n",
    "        32,\n",
    "    ),\n",
    "    dtype=np.float32,\n",
    ")\n",
    "with popxl.Session(ir, \"ipu_hw\") as session:\n",
    "    session.run({input_stream: inputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we include gradients in the example our pipelined program becomes:\n",
    "```python\n",
    "...\n",
    "# create graphs in the appropriate ipu context\n",
    "...\n",
    "with popxl.in_sequence(True):\n",
    "    with addons.pipelined_execution(steps=N) as p:\n",
    "        with p.stage(0), popxl.ipu(0):\n",
    "            # layer0 forward\n",
    "            x = ops.host_load(input_stream)\n",
    "            # we need info for activations\n",
    "            layer0_info = bound_layer0.call_with_info(x) \n",
    "            x = layer0_info.outputs[0]\n",
    "            x = x.copy_to_ipu(1)\n",
    "\n",
    "        with p.stage(1), popxl.ipu(1):\n",
    "            target = ops.host_load(target_stream)\n",
    "            # layer1 forward\n",
    "            layer1_info = bound_layer1.call_with_info(x)\n",
    "            x = layer1_info.outputs[0]\n",
    "            #loss\n",
    "            loss, dx = loss_op_with_grad(x, target)\n",
    "            ops.host_store(output_stream, loss)\n",
    "            # layer1 backward\n",
    "            layer1_activations = layer1_bwd.grad_graph_info.inputs_dict(layer1_info)\n",
    "            dx, = bound_layer1_bwd.call(dx, args=layer1_activations) \n",
    "            dx = dx.copy_to_ipu(0)\n",
    "\n",
    "        with p.stage(2), popxl.ipu(0):\n",
    "            # layer0 backward\n",
    "            layer0_activations = pipeline.stash_and_restore_activations(\n",
    "                layer0_info,\n",
    "                layer0_bwd.grad_graph_info\n",
    "            )\n",
    "            dx, = bound_layer0_bwd.call(dx, args=layer0_activations)\n",
    "    \n",
    "    # outside the pipeline context\n",
    "    with popxl.ipu(0):\n",
    "        # optimizer step for layer 0\n",
    "    with popxl.ipu(1):\n",
    "        # optimizer step for layer 1\n",
    "```\n",
    "\n",
    "## Memory stashes and recomputation\n",
    "`stash_and_restore_activations` implements the FIFO activation stash: on forward pipeline stage it stashes the values into a buffer tensor, while on backwards pipeline stage it retrieves the values from the stash.\n",
    "If you would like more control over when the `Stash` graph is executed, you can use `stash_and_restore_tensor` instead.\n",
    "\n",
    "Stashing all the activations can use a lot of memory. As explained in the introduction, we can instead choose to recompute the activations.\n",
    "To add recomputation, the `addons.transforms.recompute_graph` transform can be applied to the backward graph. In this case the backward program consists in a call to the forward followed by a call to the backward. Activations are recomputed and provided to the backward.\n",
    "When you use recomputation on a graph, you have two forward calls, one in the forward stage and one in the backward stage. In the forward stage, intermediate tensors are not used and are optimized away by popart and poplar.\n",
    "\n",
    "![ Figure 3: Recomputation ](images/recomputation.png)\n",
    "<figcaption> <b>Fig 3: </b> Recomputation transform. The backward graph is transformed to include a call to the forward graph. Activations recomputed in this forward are used to run the backward graph. <code>I</code> are intermediate tensors and apostrophes denote derivatives.\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist with pipelining\n",
    "We are now ready to implement mnist training with pipelining. We'll implement pipelining only for training, the testing program is unchanged.\n",
    "\n",
    "The network we've used so far consists of 4 layers.\n",
    "We are going to place the first two layers on the first ipu, and the last two on the second ipu.\n",
    "\n",
    "Our pipeline has 3 stages in total:\n",
    "- stage0 on ipu0 : fc1 forward, fc2 forward \n",
    "- stage1 on ipu1 : fc3 forward, fc4 forward, fc4 backward , fc3 backward\n",
    "- stage2 on ipu0 : fc2 backward, fc1 backward\n",
    "Hence, our `gradient_accumulation` factor needs to be `>= 3*2 = 6`.\n",
    "\n",
    "Note that since each layer is treated separately, we can't create the full graph using the `Net()` module, we need to build each layer with the `Linear()` module. To simplify the code, we have thus incorporated the `gelu` activation function in the linear module. For the same reason, we introduce a `ModuleGraphs` class that gathers together the forward and backward graphs for a given layer, keeps track of the variables and performs the weights update for the layer. \n",
    "\n",
    "Last thing to note is that in the [previous tutorial](../3_data_parallelism) we used \n",
    "```\n",
    "autodiff_with_accumulation(graph, tensors_to_accumulate_grads=graph.args.tensors)\n",
    "```\n",
    "Specifying `tensors_to_accumulate_grads=graph.args.tensors` means that all the weights gradient are accumulated, and the backward graph has no outputs. \n",
    "\n",
    "However, we will now use \n",
    "```\n",
    "autodiff_with_accumulation( graph,\n",
    "                            tensors_to_accumulate_grads=graph.args.tensors,\n",
    "                            grads_required=(graph.graph.inputs[0], ))\n",
    "```\n",
    "for all layers but the first one.\n",
    "The extra `grads_required=(graph.graph.inputs[0], )` means that the backward graph will return the gradient of the layer input, `dx`, each time it is called. This is needed since `dx` must be provided to previous layers for the backward to continue. In the previous tutorial it was not needed since we were differentiating the entire network.\n",
    "\n",
    "Options for training includes a `recomputation` flag. If true, the `addons.transforms.recompute_graph()` transform is applied to the backward graph of each layer.\n",
    "\n",
    "Note that `stash_and_restore_activations` is still there: even if intermediate activations are recomputed, some tensors need to be stashed and retrieved from the forward stage ( fc2 backward, for example, needs fc1 output from the previous stage to run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from functools import partial\n",
    "from typing import Mapping, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "from dataclasses import dataclass, field\n",
    "import popxl\n",
    "import popxl_addons as addons\n",
    "import popxl.ops as ops\n",
    "from typing import Union, Dict\n",
    "from popxl_addons.graph import GraphWithNamedArgs\n",
    "from popxl_addons.named_tensors import NamedTensors\n",
    "from popxl_addons.variable_factory import NamedVariableFactories\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data(test_batch_size: int, batch_size: int):\n",
    "    training_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize(\n",
    "                        (0.1307,), (0.3081,)\n",
    "                    ),  # mean and std computed on the training set.\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    validation_data = torch.utils.data.DataLoader(\n",
    "        torchvision.datasets.MNIST(\n",
    "            \"~/.torch/datasets\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=torchvision.transforms.Compose(\n",
    "                [\n",
    "                    torchvision.transforms.ToTensor(),\n",
    "                    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return training_data, validation_data\n",
    "\n",
    "\n",
    "def accuracy(predictions: np.ndarray, labels: np.ndarray):\n",
    "    ind = np.argmax(predictions, axis=-1).flatten()\n",
    "    labels = labels.detach().numpy().flatten()\n",
    "    return np.mean(ind == labels) * 100.0\n",
    "\n",
    "\n",
    "# include gelu\n",
    "class Linear(addons.Module):\n",
    "    def __init__(self, out_features: int, bias: bool = True, gelu: bool = True):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.gelu = gelu\n",
    "\n",
    "    def build(self, x: popxl.Tensor) -> popxl.Tensor:\n",
    "        # add a state variable to the module\n",
    "        w = self.add_variable_input(\n",
    "            \"weight\",\n",
    "            partial(np.random.normal, 0, 0.02, (x.shape[-1], self.out_features)),\n",
    "            x.dtype,\n",
    "        )\n",
    "        y = x @ w\n",
    "        if self.bias:\n",
    "            # add a state variable to the module\n",
    "            b = self.add_variable_input(\"bias\", partial(np.zeros, y.shape[-1]), x.dtype)\n",
    "            y = y + b\n",
    "        if self.gelu:\n",
    "            y = ops.gelu(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "# gelu now included in the layers\n",
    "class Net(addons.Module):\n",
    "    def __init__(self, cache: Optional[addons.GraphCache] = None):\n",
    "        super().__init__(cache=cache)\n",
    "        self.fc1 = Linear(512)\n",
    "        self.fc2 = Linear(512)\n",
    "        self.fc3 = Linear(512)\n",
    "        self.fc4 = Linear(10, gelu=False)\n",
    "\n",
    "    def build(self, x: popxl.Tensor):\n",
    "        x = x.reshape((-1, 28 * 28))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def evaluate_throughput(session, samples_per_step, epochs: int = 5):\n",
    "    inputs = {\n",
    "        stream: np.ones(\n",
    "            session._full_input_shape(stream.shape), stream.dtype.as_numpy()\n",
    "        )\n",
    "        for stream in session.expected_inputs()\n",
    "    }\n",
    "\n",
    "    durations = []\n",
    "    with session:\n",
    "        for i in range(epochs):\n",
    "            start = time()\n",
    "            session.run(inputs)\n",
    "            dur = time() - start\n",
    "            durations.append(dur)\n",
    "\n",
    "    duration = np.mean(durations)\n",
    "\n",
    "    result_str = (\n",
    "        f\"Mean duration: {duration} s \"\n",
    "        f\"Throughput: {samples_per_step/duration:6.1f} samples/s \"\n",
    "    )\n",
    "    print(result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Train:\n",
    "    micro_batch_size: int = 8\n",
    "    lr: Union[float, popxl.Tensor] = 1e-3\n",
    "    epochs: int = 1\n",
    "    gradient_accumulation: int = 1\n",
    "    data_parallel: int = 1\n",
    "    device = \"ipu_hw\"\n",
    "    recomputation = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Test:\n",
    "    micro_batch_size: int = 80\n",
    "    device = \"ipu_hw\"\n",
    "\n",
    "\n",
    "class Options:\n",
    "    train = Train()\n",
    "    test = Test()\n",
    "\n",
    "\n",
    "opts = Options()\n",
    "opts.train.micro_batch_size = 5\n",
    "opts.train.lr = 1e-3\n",
    "opts.train.epochs = 1\n",
    "opts.train.gradient_accumulation = (\n",
    "    6  # we will use three stages, hence we need gradient_accumulation >= 6\n",
    ")\n",
    "opts.train.data_parallel = 1\n",
    "opts.train.recomputation = False\n",
    "opts.test.micro_batch_size = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adam optimizer.\n",
    "Defines adam update step for a single variable\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Adam(addons.Module):\n",
    "    # we need to specify in_sequence because a lot of operations are in place and their order\n",
    "    # shouldn't be rearranged\n",
    "    @popxl.in_sequence()\n",
    "    def build(\n",
    "        self,\n",
    "        var: popxl.TensorByRef,\n",
    "        grad: popxl.Tensor,\n",
    "        *,\n",
    "        lr: Union[float, popxl.Tensor],\n",
    "        beta1: Union[float, popxl.Tensor] = 0.9,\n",
    "        beta2: Union[float, popxl.Tensor] = 0.999,\n",
    "        eps: Union[float, popxl.Tensor] = 1e-5,\n",
    "        weight_decay: Union[float, popxl.Tensor] = 1e-2,\n",
    "        first_order_dtype: popxl.dtype = popxl.float16,\n",
    "        bias_correction: bool = True\n",
    "    ):\n",
    "\n",
    "        # gradient estimators for the variable var - same shape as the variable\n",
    "        first_order = self.add_variable_input(\n",
    "            \"first_order\", partial(np.zeros, var.shape), first_order_dtype, by_ref=True\n",
    "        )\n",
    "        ops.var_updates.accumulate_moving_average_(first_order, grad, f=beta1)\n",
    "\n",
    "        # variance estimators for the variable var - same shape as the variable\n",
    "        second_order = self.add_variable_input(\n",
    "            \"second_order\", partial(np.zeros, var.shape), popxl.float32, by_ref=True\n",
    "        )\n",
    "        ops.var_updates.accumulate_moving_average_square_(second_order, grad, f=beta2)\n",
    "\n",
    "        # adam is a biased estimator: provide the step to correct bias\n",
    "        step = None\n",
    "        if bias_correction:\n",
    "            step = self.add_variable_input(\n",
    "                \"step\", partial(np.zeros, ()), popxl.float32, by_ref=True\n",
    "            )\n",
    "\n",
    "        # calculate the weight increment with adam heuristic\n",
    "        updater = ops.var_updates.adam_updater(\n",
    "            first_order,\n",
    "            second_order,\n",
    "            weight=var,\n",
    "            weight_decay=weight_decay,\n",
    "            time_step=step,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "            epsilon=eps,\n",
    "        )\n",
    "\n",
    "        # in place weight update: w += (-lr)*dw\n",
    "        ops.scaled_add_(var, updater, b=-lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Groups together the forward and backward graphs of a layer for easy access and handling.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Graphs:\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_name: str,\n",
    "        fwd: GraphWithNamedArgs,\n",
    "        bwd: GraphWithNamedArgs,\n",
    "        facts: NamedVariableFactories,\n",
    "        optimizer: addons.Module,\n",
    "    ):\n",
    "        self.layer_name = layer_name\n",
    "        self.fwd = fwd\n",
    "        self.bwd = bwd\n",
    "        self.facts = facts\n",
    "        self.optimizer = optimizer\n",
    "        self.vars = NamedTensors()\n",
    "\n",
    "    def init_and_bind_fwd(self):\n",
    "        self.vars.insert(\"fwd\", self.facts.fwd.init(self.layer_name))\n",
    "        return self.fwd.bind(self.vars.fwd)\n",
    "\n",
    "    def init_and_bind_bwd(self):\n",
    "        self.vars.insert(\"bwd\", self.facts.bwd.init(self.layer_name))\n",
    "        return self.bwd.bind(self.vars.bwd)\n",
    "\n",
    "    def recompute_graph(self):\n",
    "        self.bwd = addons.transforms.recompute_graph(self.bwd)\n",
    "\n",
    "    def replicated_all_reduce(self):\n",
    "        for g in self.vars.bwd.tensors[:-1]:\n",
    "            g = ops.collectives.replicated_all_reduce_(g, op=\"mean\")\n",
    "\n",
    "    def optimizer_step(self, lr: Union[float, popxl.Tensor]):\n",
    "        var_dict = self.vars.fwd.named_tensors\n",
    "        grad_dict = self.vars.bwd.accum.to_dict()\n",
    "        for name, var in var_dict.items():\n",
    "            opt_facts, opt = self.optimizer.create_graph(\n",
    "                var, var.spec, lr=lr, weight_decay=0.0, bias_correction=True\n",
    "            )\n",
    "            state = opt_facts.init()\n",
    "            opt.bind(state).call(var, grad_dict[name])\n",
    "\n",
    "        ops.var_updates.accumulator_scale_(self.vars.bwd.mean_accum_counter, 0.0)\n",
    "\n",
    "    def reset_vars(self):\n",
    "        self.vars._clear()\n",
    "\n",
    "\n",
    "def create_graphs(\n",
    "    layer_name: str,\n",
    "    layer: addons.Module,\n",
    "    optimizer: addons.module,\n",
    "    opts,\n",
    "    require_input0: bool,\n",
    "    *args,\n",
    "    **kwargs\n",
    "):\n",
    "    facts, graph = layer.create_graph(*args)\n",
    "    # tensors_to_accumulate_grads = graph.args.tensors : accumulate gradients of the weights\n",
    "    # grads_required = (graph.graph.inputs[0],): we need to return the gradient of the first input of the layer, since it\n",
    "    # will be starting value for backpropagation in the other layers\n",
    "    req_grads = (graph.graph.inputs[0],) if require_input0 else ()\n",
    "    bwd_facts, bwd_graph = addons.transforms.autodiff_with_accumulation(\n",
    "        graph, tensors_to_accumulate_grads=graph.args.tensors, grads_required=req_grads\n",
    "    )\n",
    "    factories = NamedVariableFactories()\n",
    "    factories.insert(\"fwd\", facts)\n",
    "    factories.insert(\"bwd\", bwd_facts)\n",
    "\n",
    "    return Graphs(layer_name, graph, bwd_graph, factories, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_program(opts):\n",
    "    ir = popxl.Ir()\n",
    "    ir.replication_factor = opts.train.data_parallel\n",
    "\n",
    "    with ir.main_graph:\n",
    "        # -----  Define input and output streams -----\n",
    "        img_spec = popxl.TensorSpec(\n",
    "            (opts.train.micro_batch_size, 28 * 28), popxl.float32\n",
    "        )\n",
    "        inner_spec = popxl.TensorSpec((opts.train.micro_batch_size, 512), popxl.float32)\n",
    "\n",
    "        img_stream = popxl.h2d_stream(img_spec.shape, popxl.float32, \"image\")\n",
    "        label_stream = popxl.h2d_stream(\n",
    "            (opts.train.micro_batch_size,), popxl.int32, \"labels\"\n",
    "        )\n",
    "        loss_stream = popxl.d2h_stream((), popxl.float32, \"loss\")\n",
    "\n",
    "        optimizer = Adam(cache=True)\n",
    "        steps = opts.train.gradient_accumulation\n",
    "\n",
    "        # ----- Create graphs -----\n",
    "\n",
    "        # create graphs in the appropriate ipu context\n",
    "        with popxl.ipu(0):\n",
    "            fc1 = create_graphs(\"fc1\", Linear(512), optimizer, opts, False, img_spec)\n",
    "            fc2 = create_graphs(\"fc2\", Linear(512), optimizer, opts, True, inner_spec)\n",
    "        with popxl.ipu(1):\n",
    "            fc3 = create_graphs(\"fc3\", Linear(512), optimizer, opts, True, inner_spec)\n",
    "            fc4 = create_graphs(\n",
    "                \"fc4\", Linear(10, gelu=False), optimizer, opts, True, inner_spec\n",
    "            )\n",
    "\n",
    "        # ----- Transform graphs -----\n",
    "        # for example, add recomputation\n",
    "        if opts.train.recomputation:\n",
    "            fc1.recompute_graph()\n",
    "            fc2.recompute_graph()\n",
    "            fc3.recompute_graph()\n",
    "            fc4.recompute_graph()\n",
    "\n",
    "        # ----- Construct Execution Scheme -----\n",
    "        #\n",
    "        #  Pipeline\n",
    "        #   stage0, ipu0: fc1 forward, fc2 forward\n",
    "        #   stage1, ipu1: fc3 forward, fc4 forward, loss, fc4 backward, fc3 backward\n",
    "        #   stage2, ipu0: fc2 backward, fc1 backward\n",
    "        #\n",
    "        #  Replica Reduction\n",
    "        #  Optimizer\n",
    "        #\n",
    "        # --------------------------------------\n",
    "\n",
    "        # ----- Pipeline -----\n",
    "        with popxl.in_sequence(True):\n",
    "            # context for pipeline: when the context closes the pipeline transformation is applied  to the graph\n",
    "            with addons.pipelined_execution(steps) as pipeline:\n",
    "\n",
    "                with pipeline.stage(0), popxl.ipu(0):\n",
    "                    # fc1 fc2 forward\n",
    "                    img_t = ops.host_load(img_stream)\n",
    "                    x: popxl.Tensor\n",
    "                    fc1_info = fc1.init_and_bind_fwd().call_with_info(img_t)\n",
    "                    x = fc1_info.outputs[0]\n",
    "                    fc2_info = fc2.init_and_bind_fwd().call_with_info(x)\n",
    "                    x = fc2_info.outputs[0]\n",
    "                    x = x.copy_to_ipu(1)\n",
    "\n",
    "                with pipeline.stage(1), popxl.ipu(1):\n",
    "                    # fc3 fc4 forward\n",
    "                    labels = ops.host_load(label_stream, \"labels\")\n",
    "                    fc3_info = fc3.init_and_bind_fwd().call_with_info(x)\n",
    "                    x = fc3_info.outputs[0]\n",
    "                    fc4_info = fc4.init_and_bind_fwd().call_with_info(x)\n",
    "                    x = fc4_info.outputs[0]\n",
    "\n",
    "                    # loss\n",
    "                    loss, dx = addons.ops.cross_entropy_with_grad(x, labels)\n",
    "                    ops.host_store(loss_stream, loss)\n",
    "\n",
    "                    # grads\n",
    "                    fc3_activations = fc3.bwd.grad_graph_info.inputs_dict(fc3_info)\n",
    "                    fc4_activations = fc4.bwd.grad_graph_info.inputs_dict(fc4_info)\n",
    "                    (dx,) = fc4.init_and_bind_bwd().call(\n",
    "                        dx, args=fc4_activations\n",
    "                    )  # provide fc4 activations\n",
    "                    (dx,) = fc3.init_and_bind_bwd().call(\n",
    "                        dx, args=fc3_activations\n",
    "                    )  # provide fc3 activations\n",
    "\n",
    "                    dx = dx.copy_to_ipu(0)\n",
    "\n",
    "                with pipeline.stage(2), popxl.ipu(0):\n",
    "                    # using stash_and_restore_activations ensure that when the pipeline graph is created\n",
    "                    # activations are stashed during forward and retrieved from the FIFO stash during backward.\n",
    "                    # Needs to be called inside a stage\n",
    "\n",
    "                    # grads\n",
    "                    fc2_activ = pipeline.stash_and_restore_activations(\n",
    "                        fc2_info, fc2.bwd.grad_graph_info\n",
    "                    )\n",
    "                    (dx,) = fc2.init_and_bind_bwd().call(dx, args=fc2_activ)\n",
    "\n",
    "                    fc1_activ = pipeline.stash_and_restore_activations(\n",
    "                        fc1_info, fc1.bwd.grad_graph_info\n",
    "                    )\n",
    "                    fc1.init_and_bind_bwd().call(dx, args=fc1_activ)\n",
    "\n",
    "            # -----/ Pipeline -----\n",
    "\n",
    "            # ----- Replica Reduction -----\n",
    "            if opts.train.data_parallel > 1:\n",
    "                with popxl.ipu(0):\n",
    "                    fc1.replicated_all_reduce()\n",
    "                    fc2.replicated_all_reduce()\n",
    "\n",
    "                with popxl.ipu(1):\n",
    "                    fc3.replicated_all_reduce()\n",
    "                    fc4.replicated_all_reduce()\n",
    "\n",
    "            # ----- Optimizer -----\n",
    "            with popxl.ipu(0):\n",
    "                fc1.optimizer_step(opts.train.lr)\n",
    "                fc2.optimizer_step(opts.train.lr)\n",
    "\n",
    "            with popxl.ipu(1):\n",
    "                fc3.optimizer_step(opts.train.lr)\n",
    "                fc4.optimizer_step(opts.train.lr)\n",
    "\n",
    "    # we have a for loop, the number of host loads is equal to gradient_accumulation\n",
    "    ir.num_host_transfers = opts.train.gradient_accumulation\n",
    "\n",
    "    # group all the variables to be able to copy weights to the test session.\n",
    "    # Names need to match with the Net() names used for inference\n",
    "    vars = NamedTensors()\n",
    "    vars.insert(\"fc1\", fc1.vars.fwd)\n",
    "    vars.insert(\"fc2\", fc2.vars.fwd)\n",
    "    vars.insert(\"fc3\", fc3.vars.fwd)\n",
    "    vars.insert(\"fc4\", fc4.vars.fwd)\n",
    "\n",
    "    return popxl.Session(ir, \"ipu_hw\"), [img_stream, label_stream], vars, loss_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch_size = (\n",
    "    opts.train.micro_batch_size\n",
    "    * opts.train.gradient_accumulation\n",
    "    * opts.train.data_parallel\n",
    ")\n",
    "training_data, test_data = get_mnist_data(opts.test.micro_batch_size, global_batch_size)\n",
    "train_session, train_input_streams, train_variables, loss_stream = train_program(opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_session:\n",
    "    nr_batches = len(training_data)\n",
    "    for epoch in range(1, opts.train.epochs + 1):\n",
    "        nr_batches = len(training_data)\n",
    "        for epoch in range(1, opts.train.epochs + 1):\n",
    "            print(\"Epoch {0}/{1}\".format(opts.train.epochs, opts.train.epochs))\n",
    "            bar = tqdm(training_data, total=nr_batches)\n",
    "            for data, labels in bar:\n",
    "                # reshape data accounting for replication and num hosts transfers\n",
    "                data = data.reshape(\n",
    "                    train_session.ir.num_host_transfers,\n",
    "                    train_session.ir.replication_factor,\n",
    "                    opts.train.micro_batch_size,\n",
    "                    28 * 28,\n",
    "                ).squeeze()\n",
    "                labels = labels.reshape(\n",
    "                    train_session.ir.num_host_transfers,\n",
    "                    train_session.ir.replication_factor,\n",
    "                    opts.train.micro_batch_size,\n",
    "                ).squeeze()\n",
    "\n",
    "                inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "                    zip(train_input_streams, [data.squeeze().float(), labels.int()])\n",
    "                )\n",
    "                loss = train_session.run(inputs)\n",
    "                losses_np = loss[\n",
    "                    loss_stream\n",
    "                ]  # shape(ir.num_host_transfers, ir.replication_factor, )\n",
    "                avg_loss = np.mean(losses_np)\n",
    "                bar.set_description(\"Loss:{:0.4f}\".format(avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get weights data : dictionary { train_session variables : tensor data (numpy) }\n",
    "train_vars_to_data = train_session.get_tensors_data(train_variables.tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Throughput and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_program(test_batch_size, device):\n",
    "    ir = popxl.Ir(replication=1)\n",
    "    with ir.main_graph:\n",
    "        # Inputs\n",
    "        in_stream = popxl.h2d_stream((test_batch_size, 28, 28), popxl.float32, \"image\")\n",
    "        in_t = ops.host_load(in_stream)\n",
    "\n",
    "        # Create graphs\n",
    "        facts, graph = Net().create_graph(in_t)\n",
    "\n",
    "        # Initialise variables\n",
    "        variables = facts.init()\n",
    "\n",
    "        # Forward\n",
    "        (outputs,) = graph.bind(variables).call(in_t)\n",
    "        out_stream = popxl.d2h_stream(outputs.shape, outputs.dtype, \"outputs\")\n",
    "        ops.host_store(out_stream, outputs)\n",
    "\n",
    "    ir.num_host_transfers = 1\n",
    "    return popxl.Session(ir, device), [in_stream], variables, out_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test program and test session\n",
    "test_session, test_input_streams, test_variables, out_stream = test_program(\n",
    "    opts.test.micro_batch_size, opts.test.device\n",
    ")\n",
    "\n",
    "# dictionary { train_session variables : test_session variables }\n",
    "train_vars_to_test_vars = train_variables.to_mapping(test_variables)\n",
    "\n",
    "# Create a dictionary { test_session variables : tensor data (numpy) }\n",
    "# We want to copy the values before evaluating throughput on synthetic data, otherwise weights are changed\n",
    "test_vars_to_data = {\n",
    "    test_var: train_vars_to_data[train_var].copy()\n",
    "    for train_var, test_var in train_vars_to_test_vars.items()\n",
    "}\n",
    "\n",
    "# Copy trained weights to the program, with a single host to device transfer at the end\n",
    "test_session.write_variables_data(test_vars_to_data)\n",
    "\n",
    "# evaluate the ratio samples per step / time for train session\n",
    "print(\"train session\")\n",
    "evaluate_throughput(train_session, global_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_batches = len(test_data)\n",
    "sum_acc = 0.0\n",
    "with test_session:\n",
    "    for data, labels in tqdm(test_data, total=nr_batches):\n",
    "        inputs: Mapping[popxl.HostToDeviceStream, np.ndarray] = dict(\n",
    "            zip(test_input_streams, [data.squeeze().float(), labels.int()])\n",
    "        )\n",
    "        output = test_session.run(inputs)\n",
    "        sum_acc += accuracy(output[out_stream], labels)\n",
    "print(\"Accuracy on test set: {:0.2f}%\".format(sum_acc / len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_step = (\n",
    "    opts.test.micro_batch_size\n",
    ")  # no data parallelism or gradient accumulation for inference in this program\n",
    "print(\"test session\")\n",
    "evaluate_throughput(test_session, samples_per_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
